<html><head>
    <title>NeurIPS.2025 - Oral | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for NeurIPS.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">NeurIPS.2025 - Oral</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 77
    </p>
    <div class="papers">
        <div id="KurYdcCbjv@OpenReview" class="panel paper" keywords="permutations,transformers,connectivity,neuron,loss,barrier,reparameterizations,linear,mode,independently">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KurYdcCbjv" target="_blank" title="1/77"><span class="index notranslate">#1</span></a>
                <a id="title-KurYdcCbjv@OpenReview" class="title-link" href="/venue/KurYdcCbjv@OpenReview" target="_blank">Generalized Linear Mode Connectivity for Transformers</a>
                <a id="pdf-KurYdcCbjv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KurYdcCbjv@OpenReview', this)" data="https://openreview.net/pdf?id=KurYdcCbjv">[PDF<sup id="pdf-stars-KurYdcCbjv@OpenReview">110</sup>]</a>
                <a id="copy-KurYdcCbjv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KurYdcCbjv@OpenReview')">[Copy]</a>
                <a id="kimi-KurYdcCbjv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KurYdcCbjv@OpenReview', this)">[Kimi<sup id="kimi-stars-KurYdcCbjv@OpenReview">127</sup>]</a>
                <a id="rel-KurYdcCbjv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KurYdcCbjv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KurYdcCbjv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Theus" target="_blank">Alexander Theus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Cabodi" target="_blank">Alessandro Cabodi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sotiris Anagnostidis" target="_blank">Sotiris Anagnostidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antonio Orvieto" target="_blank">Antonio Orvieto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sidak Pal Singh" target="_blank">Sidak Pal Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentina Boeva" target="_blank">Valentina Boeva</a>
            </p>
            <p id="summary-KurYdcCbjv@OpenReview" class="summary">Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;linear mode connectivity&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 12.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1010.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="texatom" id="MathJax-Span-3"><span class="mrow" id="MathJax-Span-4"><span class="mtext" id="MathJax-Span-5" style="font-family: MathJax_Main-italic;">linear mode connectivity</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">linear mode connectivity</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-1">\textit{linear mode connectivity}</script> (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.</p>
            <p id="subjects-KurYdcCbjv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-KurYdcCbjv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KurYdcCbjv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KurYdcCbjv@OpenReview" onclick="foldPdfKimi('KurYdcCbjv@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="jzPQRbGkAq@OpenReview" class="panel paper" keywords="motion,clips,phase,semantically,tpdm,diffusion,spdm,compositional,transitional,sequences">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jzPQRbGkAq" target="_blank" title="2/77"><span class="index notranslate">#2</span></a>
                <a id="title-jzPQRbGkAq@OpenReview" class="title-link" href="/venue/jzPQRbGkAq@OpenReview" target="_blank">Deep Compositional Phase Diffusion for Long Motion Sequence Generation</a>
                <a id="pdf-jzPQRbGkAq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jzPQRbGkAq@OpenReview', this)" data="https://openreview.net/pdf?id=jzPQRbGkAq">[PDF<sup id="pdf-stars-jzPQRbGkAq@OpenReview">52</sup>]</a>
                <a id="copy-jzPQRbGkAq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jzPQRbGkAq@OpenReview')">[Copy]</a>
                <a id="kimi-jzPQRbGkAq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jzPQRbGkAq@OpenReview', this)">[Kimi<sup id="kimi-stars-jzPQRbGkAq@OpenReview">51</sup>]</a>
                <a id="rel-jzPQRbGkAq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jzPQRbGkAq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jzPQRbGkAq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ho Yin Au" target="_blank">Ho Yin Au</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Chen" target="_blank">Jie Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junkun Jiang" target="_blank">Junkun Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyu Xiang" target="_blank">Jingyu Xiang</a>
            </p>
            <p id="summary-jzPQRbGkAq@OpenReview" class="summary">Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.</p>
            <p id="subjects-jzPQRbGkAq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-jzPQRbGkAq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jzPQRbGkAq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jzPQRbGkAq@OpenReview" onclick="foldPdfKimi('jzPQRbGkAq@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="eafIjoZAHm@OpenReview" class="panel paper" keywords="gnnxemplar,exemplars,global,rules,gnn,exemplar,explanations,interpretability,language,predictions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eafIjoZAHm" target="_blank" title="3/77"><span class="index notranslate">#3</span></a>
                <a id="title-eafIjoZAHm@OpenReview" class="title-link" href="/venue/eafIjoZAHm@OpenReview" target="_blank">GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability</a>
                <a id="pdf-eafIjoZAHm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eafIjoZAHm@OpenReview', this)" data="https://openreview.net/pdf?id=eafIjoZAHm">[PDF<sup id="pdf-stars-eafIjoZAHm@OpenReview">20</sup>]</a>
                <a id="copy-eafIjoZAHm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eafIjoZAHm@OpenReview')">[Copy]</a>
                <a id="kimi-eafIjoZAHm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eafIjoZAHm@OpenReview', this)">[Kimi<sup id="kimi-stars-eafIjoZAHm@OpenReview">30</sup>]</a>
                <a id="rel-eafIjoZAHm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eafIjoZAHm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eafIjoZAHm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Burouj Armgaan" target="_blank">Burouj Armgaan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eshan Jain" target="_blank">Eshan Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harsh Pandey" target="_blank">Harsh Pandey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahesh Chandran" target="_blank">Mahesh Chandran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayan Ranu" target="_blank">Sayan Ranu</a>
            </p>
            <p id="summary-eafIjoZAHm@OpenReview" class="summary">Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-7"><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">k</script>-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.</p>
            <p id="subjects-eafIjoZAHm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-eafIjoZAHm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eafIjoZAHm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eafIjoZAHm@OpenReview" onclick="foldPdfKimi('eafIjoZAHm@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="tirl2l9oKg@OpenReview" class="panel paper" keywords="graph,rag4gfm,retrieval,gfms,indexing,gfm,knowledge,faithfulness,foundation,updating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tirl2l9oKg" target="_blank" title="4/77"><span class="index notranslate">#4</span></a>
                <a id="title-tirl2l9oKg@OpenReview" class="title-link" href="/venue/tirl2l9oKg@OpenReview" target="_blank">RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation</a>
                <a id="pdf-tirl2l9oKg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tirl2l9oKg@OpenReview', this)" data="https://openreview.net/pdf?id=tirl2l9oKg">[PDF<sup id="pdf-stars-tirl2l9oKg@OpenReview">32</sup>]</a>
                <a id="copy-tirl2l9oKg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tirl2l9oKg@OpenReview')">[Copy]</a>
                <a id="kimi-tirl2l9oKg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tirl2l9oKg@OpenReview', this)">[Kimi<sup id="kimi-stars-tirl2l9oKg@OpenReview">39</sup>]</a>
                <a id="rel-tirl2l9oKg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tirl2l9oKg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tirl2l9oKg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingliang Wang" target="_blank">Xingliang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zemin Liu" target="_blank">Zemin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxiao Han" target="_blank">Junxiao Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuiguang Deng" target="_blank">Shuiguang Deng</a>
            </p>
            <p id="summary-tirl2l9oKg@OpenReview" class="summary">Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. RAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. The graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference. Extensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\footnote{Code: \url{https://github.com/Matrixmax/RAG4GFM}.}.</p>
            <p id="subjects-tirl2l9oKg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-tirl2l9oKg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tirl2l9oKg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tirl2l9oKg@OpenReview" onclick="foldPdfKimi('tirl2l9oKg@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="XPe55Uffd7@OpenReview" class="panel paper" keywords="agnostic,active,passive,learning,complexity,always,leading,term,query,sharply">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XPe55Uffd7" target="_blank" title="5/77"><span class="index notranslate">#5</span></a>
                <a id="title-XPe55Uffd7@OpenReview" class="title-link" href="/venue/XPe55Uffd7@OpenReview" target="_blank">Agnostic Active Learning Is Always Better Than Passive Learning</a>
                <a id="pdf-XPe55Uffd7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XPe55Uffd7@OpenReview', this)" data="https://openreview.net/pdf?id=XPe55Uffd7">[PDF<sup id="pdf-stars-XPe55Uffd7@OpenReview">32</sup>]</a>
                <a id="copy-XPe55Uffd7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XPe55Uffd7@OpenReview')">[Copy]</a>
                <a id="kimi-XPe55Uffd7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XPe55Uffd7@OpenReview', this)">[Kimi<sup id="kimi-stars-XPe55Uffd7@OpenReview">44</sup>]</a>
                <a id="rel-XPe55Uffd7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XPe55Uffd7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XPe55Uffd7@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Steve Hanneke" target="_blank">Steve Hanneke</a>
            </p>
            <p id="summary-XPe55Uffd7@OpenReview" class="summary">We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;every&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-9" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1002.35em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-10"><span class="texatom" id="MathJax-Span-11"><span class="mrow" id="MathJax-Span-12"><span class="mtext" id="MathJax-Span-13" style="font-family: MathJax_Main-italic;">every</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">every</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-3">\textit{every}</script> concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning.</p>
            <p id="subjects-XPe55Uffd7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-XPe55Uffd7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XPe55Uffd7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XPe55Uffd7@OpenReview" onclick="foldPdfKimi('XPe55Uffd7@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="QN0E0KX2LM@OpenReview" class="panel paper" keywords="learnability,attention,linear,expressivity,polynomial,transformers,head,learning,transformer,rkhs">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QN0E0KX2LM" target="_blank" title="6/77"><span class="index notranslate">#6</span></a>
                <a id="title-QN0E0KX2LM@OpenReview" class="title-link" href="/venue/QN0E0KX2LM@OpenReview" target="_blank">Learning Linear Attention in Polynomial Time</a>
                <a id="pdf-QN0E0KX2LM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QN0E0KX2LM@OpenReview', this)" data="https://openreview.net/pdf?id=QN0E0KX2LM">[PDF<sup id="pdf-stars-QN0E0KX2LM@OpenReview">35</sup>]</a>
                <a id="copy-QN0E0KX2LM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QN0E0KX2LM@OpenReview')">[Copy]</a>
                <a id="kimi-QN0E0KX2LM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QN0E0KX2LM@OpenReview', this)">[Kimi<sup id="kimi-stars-QN0E0KX2LM@OpenReview">40</sup>]</a>
                <a id="rel-QN0E0KX2LM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QN0E0KX2LM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QN0E0KX2LM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Morris Yau" target="_blank">Morris Yau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ekin Akyürek" target="_blank">Ekin Akyürek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayuan Mao" target="_blank">Jiayuan Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua B. Tenenbaum" target="_blank">Joshua B. Tenenbaum</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefanie Jegelka" target="_blank">Stefanie Jegelka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Andreas" target="_blank">Jacob Andreas</a>
            </p>
            <p id="summary-QN0E0KX2LM@OpenReview" class="summary">Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS. Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization. We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models.</p>
            <p id="subjects-QN0E0KX2LM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-QN0E0KX2LM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QN0E0KX2LM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QN0E0KX2LM@OpenReview" onclick="foldPdfKimi('QN0E0KX2LM@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="EoebmBe9fG@OpenReview" class="panel paper" keywords="transductive,mistake,online,littlestone,bound,omega,log,ben,david,1997">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EoebmBe9fG" target="_blank" title="7/77"><span class="index notranslate">#7</span></a>
                <a id="title-EoebmBe9fG@OpenReview" class="title-link" href="/venue/EoebmBe9fG@OpenReview" target="_blank">Optimal Mistake Bounds for Transductive Online Learning</a>
                <a id="pdf-EoebmBe9fG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EoebmBe9fG@OpenReview', this)" data="https://openreview.net/pdf?id=EoebmBe9fG">[PDF<sup id="pdf-stars-EoebmBe9fG@OpenReview">13</sup>]</a>
                <a id="copy-EoebmBe9fG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EoebmBe9fG@OpenReview')">[Copy]</a>
                <a id="kimi-EoebmBe9fG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EoebmBe9fG@OpenReview', this)">[Kimi<sup id="kimi-stars-EoebmBe9fG@OpenReview">19</sup>]</a>
                <a id="rel-EoebmBe9fG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EoebmBe9fG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EoebmBe9fG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zachary Chase" target="_blank">Zachary Chase</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steve Hanneke" target="_blank">Steve Hanneke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shay Moran" target="_blank">Shay Moran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Shafer" target="_blank">Jonathan Shafer</a>
            </p>
            <p id="summary-EoebmBe9fG@OpenReview" class="summary">We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-14" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-15"><span class="texatom" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Caligraphic;">H</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4">\mathcal{H}</script> with Littlestone dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-5">d</script>, the transductive mistake bound is at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 3.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.76em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="mi" id="MathJax-Span-24" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-25" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-26"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.52em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.154em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-29" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msqrt><mi>d</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">\Omega(\sqrt{d})</script>. This establishes an exponential improvement over previous lower bounds of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-30" style="width: 5.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.79em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-31"><span class="mi" id="MathJax-Span-32" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-33" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-34" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-35"></span><span class="mi" id="MathJax-Span-36" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-37"></span><span class="mi" id="MathJax-Span-38" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-39" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7">\Omega(\log \log d)</script>, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-40" style="width: 5.419em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.482em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.38em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-41"><span class="mi" id="MathJax-Span-42" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-43" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-44"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.98em, 2.503em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-45"><span class="mi" id="MathJax-Span-46" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-47"></span><span class="mi" id="MathJax-Span-48" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1001.93em, 3.961em, -999.997em); top: -4.581em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.253em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.367em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.836em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-49" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msqrt><mi>log</mi><mo>⁡</mo><mi>d</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-8">\Omega(\sqrt{\log d})</script>, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-50" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.34em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-54" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-55"></span><span class="mi" id="MathJax-Span-56" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-57" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-9">\Omega(\log d)</script>, respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-58" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-59"><span class="mi" id="MathJax-Span-60" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-10">d</script>, there exists a class of Littlestone dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-61" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-62"><span class="mi" id="MathJax-Span-63" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-11">d</script> with transductive mistake bound <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-64" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.82em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-65"><span class="mi" id="MathJax-Span-66" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-67" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-68"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-69"><span class="mi" id="MathJax-Span-70" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.52em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.154em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-71" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>d</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-12">O(\sqrt{d})</script>. Our upper bound also improves the previous best known upper bound of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-72" style="width: 4.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.49em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-73"><span class="mo" id="MathJax-Span-74" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-75" style="font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-76"><span class="mrow" id="MathJax-Span-77"><span class="mo" id="MathJax-Span-78" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-79" style="font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-80" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-81" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="mi" id="MathJax-Span-82" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn><mo stretchy="false">)</mo><mo>⋅</mo><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-13">(2/3) \cdot d</script> from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.</p>
            <p id="subjects-EoebmBe9fG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-EoebmBe9fG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EoebmBe9fG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EoebmBe9fG@OpenReview" onclick="foldPdfKimi('EoebmBe9fG@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="rtG7n93Ru8@OpenReview" class="panel paper" keywords="entropy,regularization,robustness,policy,reinforcement,state,robust,guarantees,rollouts,standpoint">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rtG7n93Ru8" target="_blank" title="8/77"><span class="index notranslate">#8</span></a>
                <a id="title-rtG7n93Ru8@OpenReview" class="title-link" href="/venue/rtG7n93Ru8@OpenReview" target="_blank">State Entropy Regularization for Robust Reinforcement Learning</a>
                <a id="pdf-rtG7n93Ru8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rtG7n93Ru8@OpenReview', this)" data="https://openreview.net/pdf?id=rtG7n93Ru8">[PDF<sup id="pdf-stars-rtG7n93Ru8@OpenReview">24</sup>]</a>
                <a id="copy-rtG7n93Ru8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rtG7n93Ru8@OpenReview')">[Copy]</a>
                <a id="kimi-rtG7n93Ru8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rtG7n93Ru8@OpenReview', this)">[Kimi<sup id="kimi-stars-rtG7n93Ru8@OpenReview">31</sup>]</a>
                <a id="rel-rtG7n93Ru8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rtG7n93Ru8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rtG7n93Ru8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yonatan Ashlag" target="_blank">Yonatan Ashlag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Uri Koren" target="_blank">Uri Koren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mirco Mutti" target="_blank">Mirco Mutti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Esther Derman" target="_blank">Esther Derman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre-Luc Bacon" target="_blank">Pierre-Luc Bacon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shie Mannor" target="_blank">Shie Mannor</a>
            </p>
            <p id="summary-rtG7n93Ru8@OpenReview" class="summary">State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation.</p>
            <p id="subjects-rtG7n93Ru8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-rtG7n93Ru8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rtG7n93Ru8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rtG7n93Ru8@OpenReview" onclick="foldPdfKimi('rtG7n93Ru8@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="kVz9uvqUna@OpenReview" class="panel paper" keywords="matching,flow,closed,stochasticity,loss,form,generalization,stochastic,nature,arise">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kVz9uvqUna" target="_blank" title="9/77"><span class="index notranslate">#9</span></a>
                <a id="title-kVz9uvqUna@OpenReview" class="title-link" href="/venue/kVz9uvqUna@OpenReview" target="_blank">On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity</a>
                <a id="pdf-kVz9uvqUna@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kVz9uvqUna@OpenReview', this)" data="https://openreview.net/pdf?id=kVz9uvqUna">[PDF<sup id="pdf-stars-kVz9uvqUna@OpenReview">24</sup>]</a>
                <a id="copy-kVz9uvqUna@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kVz9uvqUna@OpenReview')">[Copy]</a>
                <a id="kimi-kVz9uvqUna@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kVz9uvqUna@OpenReview', this)">[Kimi<sup id="kimi-stars-kVz9uvqUna@OpenReview">17</sup>]</a>
                <a id="rel-kVz9uvqUna@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kVz9uvqUna@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kVz9uvqUna@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Quentin Bertrand" target="_blank">Quentin Bertrand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anne Gagneux" target="_blank">Anne Gagneux</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathurin Massias" target="_blank">Mathurin Massias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rémi Emonet" target="_blank">Rémi Emonet</a>
            </p>
            <p id="summary-kVz9uvqUna@OpenReview" class="summary">Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance.</p>
            <p id="subjects-kVz9uvqUna@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-kVz9uvqUna@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kVz9uvqUna@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kVz9uvqUna@OpenReview" onclick="foldPdfKimi('kVz9uvqUna@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="BSZqpqgqM0@OpenReview" class="panel paper" keywords="memorization,training,tau,mathrm,mem,gen,memorize,implicit,regularization,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BSZqpqgqM0" target="_blank" title="10/77"><span class="index notranslate">#10</span></a>
                <a id="title-BSZqpqgqM0@OpenReview" class="title-link" href="/venue/BSZqpqgqM0@OpenReview" target="_blank">Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training</a>
                <a id="pdf-BSZqpqgqM0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BSZqpqgqM0@OpenReview', this)" data="https://openreview.net/pdf?id=BSZqpqgqM0">[PDF<sup id="pdf-stars-BSZqpqgqM0@OpenReview">30</sup>]</a>
                <a id="copy-BSZqpqgqM0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BSZqpqgqM0@OpenReview')">[Copy]</a>
                <a id="kimi-BSZqpqgqM0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BSZqpqgqM0@OpenReview', this)">[Kimi<sup id="kimi-stars-BSZqpqgqM0@OpenReview">23</sup>]</a>
                <a id="rel-BSZqpqgqM0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BSZqpqgqM0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BSZqpqgqM0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tony Bonnaire" target="_blank">Tony Bonnaire</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Raphaël Urfin" target="_blank">Raphaël Urfin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giulio Biroli" target="_blank">Giulio Biroli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Mezard" target="_blank">Marc Mezard</a>
            </p>
            <p id="summary-BSZqpqgqM0@OpenReview" class="summary">Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03C4;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-83" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.57em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-84"><span class="msubsup" id="MathJax-Span-85"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-86" style="font-family: MathJax_Math-italic;">τ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="texatom" id="MathJax-Span-87"><span class="mrow" id="MathJax-Span-88"><span class="mi" id="MathJax-Span-89" style="font-size: 70.7%; font-family: MathJax_Main;">g</span><span class="mi" id="MathJax-Span-90" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-91" style="font-size: 70.7%; font-family: MathJax_Main;">n</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>τ</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-14">\tau_\mathrm{gen}</script> at which models begin to generate high-quality samples, and a later time <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03C4;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-92" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1002.03em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-93"><span class="msubsup" id="MathJax-Span-94"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-95" style="font-family: MathJax_Math-italic;">τ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="texatom" id="MathJax-Span-96"><span class="mrow" id="MathJax-Span-97"><span class="mi" id="MathJax-Span-98" style="font-size: 70.7%; font-family: MathJax_Main;">m</span><span class="mi" id="MathJax-Span-99" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-100" style="font-size: 70.7%; font-family: MathJax_Main;">m</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>τ</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">m</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-15">\tau_\mathrm{mem}</script> beyond which memorization emerges. Crucially, we find that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03C4;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-101" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1002.03em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-102"><span class="msubsup" id="MathJax-Span-103"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-104" style="font-family: MathJax_Math-italic;">τ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="texatom" id="MathJax-Span-105"><span class="mrow" id="MathJax-Span-106"><span class="mi" id="MathJax-Span-107" style="font-size: 70.7%; font-family: MathJax_Main;">m</span><span class="mi" id="MathJax-Span-108" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-109" style="font-size: 70.7%; font-family: MathJax_Main;">m</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>τ</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">m</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-16">\tau_\mathrm{mem}</script> increases linearly with the training set size <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-110" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-111"><span class="mi" id="MathJax-Span-112" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">n</script>, while <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03C4;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-113" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1001.57em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-114"><span class="msubsup" id="MathJax-Span-115"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-116" style="font-family: MathJax_Math-italic;">τ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="texatom" id="MathJax-Span-117"><span class="mrow" id="MathJax-Span-118"><span class="mi" id="MathJax-Span-119" style="font-size: 70.7%; font-family: MathJax_Main;">g</span><span class="mi" id="MathJax-Span-120" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-121" style="font-size: 70.7%; font-family: MathJax_Main;">n</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>τ</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-18">\tau_\mathrm{gen}</script> remains constant. This creates a growing window of training times with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-122" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-123"><span class="mi" id="MathJax-Span-124" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-19">n</script> where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-125" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-126"><span class="mi" id="MathJax-Span-127" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-20">n</script> becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.</p>
            <p id="subjects-BSZqpqgqM0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-BSZqpqgqM0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BSZqpqgqM0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BSZqpqgqM0@OpenReview" onclick="foldPdfKimi('BSZqpqgqM0@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="rMhQBlhh4c@OpenReview" class="panel paper" keywords="asbs,adjoint,sampler,samplers,bridge,schrödinger,diffusion,boltzmann,target,sampling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rMhQBlhh4c" target="_blank" title="11/77"><span class="index notranslate">#11</span></a>
                <a id="title-rMhQBlhh4c@OpenReview" class="title-link" href="/venue/rMhQBlhh4c@OpenReview" target="_blank">Adjoint Schrödinger Bridge Sampler</a>
                <a id="pdf-rMhQBlhh4c@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rMhQBlhh4c@OpenReview', this)" data="https://openreview.net/pdf?id=rMhQBlhh4c">[PDF<sup id="pdf-stars-rMhQBlhh4c@OpenReview">16</sup>]</a>
                <a id="copy-rMhQBlhh4c@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rMhQBlhh4c@OpenReview')">[Copy]</a>
                <a id="kimi-rMhQBlhh4c@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rMhQBlhh4c@OpenReview', this)">[Kimi<sup id="kimi-stars-rMhQBlhh4c@OpenReview">13</sup>]</a>
                <a id="rel-rMhQBlhh4c@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rMhQBlhh4c@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rMhQBlhh4c@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guan-Horng Liu" target="_blank">Guan-Horng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaemoo Choi" target="_blank">Jaemoo Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxin Chen" target="_blank">Yongxin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Kurt Miller" target="_blank">Benjamin Kurt Miller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricky T. Q. Chen" target="_blank">Ricky T. Q. Chen</a>
            </p>
            <p id="summary-rMhQBlhh4c@OpenReview" class="summary">Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers</p>
            <p id="subjects-rMhQBlhh4c@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-rMhQBlhh4c@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rMhQBlhh4c@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rMhQBlhh4c@OpenReview" onclick="foldPdfKimi('rMhQBlhh4c@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="RxkCwOKVKa@OpenReview" class="panel paper" keywords="ceiling,inference,breaking,reinforcement,strategies,execution,performance,60k,countless,complex">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RxkCwOKVKa" target="_blank" title="12/77"><span class="index notranslate">#12</span></a>
                <a id="title-RxkCwOKVKa@OpenReview" class="title-link" href="/venue/RxkCwOKVKa@OpenReview" target="_blank">Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</a>
                <a id="pdf-RxkCwOKVKa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RxkCwOKVKa@OpenReview', this)" data="https://openreview.net/pdf?id=RxkCwOKVKa">[PDF<sup id="pdf-stars-RxkCwOKVKa@OpenReview">33</sup>]</a>
                <a id="copy-RxkCwOKVKa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RxkCwOKVKa@OpenReview')">[Copy]</a>
                <a id="kimi-RxkCwOKVKa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RxkCwOKVKa@OpenReview', this)">[Kimi<sup id="kimi-stars-RxkCwOKVKa@OpenReview">32</sup>]</a>
                <a id="rel-RxkCwOKVKa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RxkCwOKVKa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RxkCwOKVKa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Chalumeau" target="_blank">Felix Chalumeau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Rajaonarivonivelomanantsoa" target="_blank">Daniel Rajaonarivonivelomanantsoa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruan John de Kock" target="_blank">Ruan John de Kock</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Claude Formanek" target="_blank">Juan Claude Formanek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sasha Abramowitz" target="_blank">Sasha Abramowitz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omayma Mahjoub" target="_blank">Omayma Mahjoub</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wiem Khlifi" target="_blank">Wiem Khlifi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Verster Du Toit" target="_blank">Simon Verster Du Toit</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Louay Ben Nessir" target="_blank">Louay Ben Nessir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Refiloe Shabe" target="_blank">Refiloe Shabe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arnol Manuel Fokam" target="_blank">Arnol Manuel Fokam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddarth Singh" target="_blank">Siddarth Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ulrich Armel Mbou Sob" target="_blank">Ulrich Armel Mbou Sob</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arnu Pretorius" target="_blank">Arnu Pretorius</a>
            </p>
            <p id="summary-RxkCwOKVKa@OpenReview" class="summary">Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available.</p>
            <p id="subjects-RxkCwOKVKa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-RxkCwOKVKa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RxkCwOKVKa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RxkCwOKVKa@OpenReview" onclick="foldPdfKimi('RxkCwOKVKa@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="UVDihUz0iT@OpenReview" class="panel paper" keywords="vert,epsilon,calibration,regret,cdot,forecasts,rounds,exp,swap,online">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UVDihUz0iT" target="_blank" title="13/77"><span class="index notranslate">#13</span></a>
                <a id="title-UVDihUz0iT@OpenReview" class="title-link" href="/venue/UVDihUz0iT@OpenReview" target="_blank">High-Dimensional Calibration from Swap Regret</a>
                <a id="pdf-UVDihUz0iT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UVDihUz0iT@OpenReview', this)" data="https://openreview.net/pdf?id=UVDihUz0iT">[PDF<sup id="pdf-stars-UVDihUz0iT@OpenReview">4</sup>]</a>
                <a id="copy-UVDihUz0iT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UVDihUz0iT@OpenReview')">[Copy]</a>
                <a id="kimi-UVDihUz0iT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UVDihUz0iT@OpenReview', this)">[Kimi<sup id="kimi-stars-UVDihUz0iT@OpenReview">13</sup>]</a>
                <a id="rel-UVDihUz0iT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UVDihUz0iT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UVDihUz0iT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maxwell Fishelson" target="_blank">Maxwell Fishelson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Golowich" target="_blank">Noah Golowich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mehryar Mohri" target="_blank">Mehryar Mohri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jon Schneider" target="_blank">Jon Schneider</a>
            </p>
            <p id="summary-UVDihUz0iT@OpenReview" class="summary">We study the online calibration of multi-dimensional forecasts over an arbitrary convex set <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2282;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-128" style="width: 3.909em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.23em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-129"><span class="texatom" id="MathJax-Span-130"><span class="mrow" id="MathJax-Span-131"><span class="mi" id="MathJax-Span-132" style="font-family: MathJax_Caligraphic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span></span><span class="mo" id="MathJax-Span-133" style="font-family: MathJax_Main; padding-left: 0.263em;">⊂</span><span class="msubsup" id="MathJax-Span-134" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.607em, -999.997em); top: -2.445em; left: 0em;"><span class="texatom" id="MathJax-Span-135"><span class="mrow" id="MathJax-Span-136"><span class="mi" id="MathJax-Span-137" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.732em;"><span class="mi" id="MathJax-Span-138" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">P</mi></mrow><mo>⊂</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-21">\mathcal{P} \subset \mathbb{R}^d</script> relative to an arbitrary norm <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-139" style="width: 2.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-140"><span class="mo" id="MathJax-Span-141" style="font-family: MathJax_Main;">∥</span><span class="mo" id="MathJax-Span-142" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="mo" id="MathJax-Span-143" style="font-family: MathJax_Main; padding-left: 0.211em;">∥</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">‖</mo><mo>⋅</mo><mo fence="false" stretchy="false">‖</mo></math></span></span><script type="math/tex" id="MathJax-Element-22">\Vert\cdot\Vert</script>. We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;&amp;#x03C1;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-144" style="width: 4.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1003.65em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-145"><span class="mi" id="MathJax-Span-146" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-147" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-148"><span style="display: inline-block; position: relative; width: 2.19em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.2em, 2.503em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-149"><span class="mi" id="MathJax-Span-150" style="font-family: MathJax_Math-italic;">ρ</span><span class="mi" id="MathJax-Span-151" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1001.15em, 3.961em, -999.997em); top: -4.581em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.471em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.211em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-152" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>ρ</mi><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-23">O(\sqrt{\rho T})</script> worst-case regret after <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-153" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-154"><span class="mi" id="MathJax-Span-155" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-24">T</script> rounds when actions are drawn from <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-156" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-157"><span class="texatom" id="MathJax-Span-158"><span class="mrow" id="MathJax-Span-159"><span class="mi" id="MathJax-Span-160" style="font-family: MathJax_Caligraphic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">P</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-25">\mathcal{P}</script> and losses are drawn from the dual <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-161" style="width: 2.607em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.14em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-162"><span class="mo" id="MathJax-Span-163" style="font-family: MathJax_Main;">∥</span><span class="mo" id="MathJax-Span-164" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="msubsup" id="MathJax-Span-165" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.37em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-166" style="font-family: MathJax_Main;">∥</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.523em;"><span class="mo" id="MathJax-Span-167" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">‖</mo><mo>⋅</mo><msub><mo fence="false" stretchy="false">‖</mo><mo>∗</mo></msub></math></span></span><script type="math/tex" id="MathJax-Element-26">\Vert \cdot \Vert_*</script> unit norm ball, then it is also possible to obtain <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-168" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-169"><span class="mi" id="MathJax-Span-170" style="font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">\epsilon</script>-calibrated forecasts after <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03C1;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-171" style="width: 9.273em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1007.61em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-172"><span class="mi" id="MathJax-Span-173" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-174" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mi" id="MathJax-Span-175" style="font-family: MathJax_Main; padding-left: 0.263em;">exp</span><span class="mo" id="MathJax-Span-176"></span><span class="mo" id="MathJax-Span-177" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-178" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-179" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-180" style="font-family: MathJax_Math-italic;">ρ</span><span class="texatom" id="MathJax-Span-181"><span class="mrow" id="MathJax-Span-182"><span class="mo" id="MathJax-Span-183" style="font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-184"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-185" style="font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="mn" id="MathJax-Span-186" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-187" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-188" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>O</mi><mo stretchy="false">(</mo><mi>ρ</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-28">T = \exp(O(\rho /\epsilon^2))</script> rounds. When <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-189" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-190"><span class="texatom" id="MathJax-Span-191"><span class="mrow" id="MathJax-Span-192"><span class="mi" id="MathJax-Span-193" style="font-family: MathJax_Caligraphic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">P</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-29">\mathcal{P}</script> is the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-194" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-195"><span class="mi" id="MathJax-Span-196" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-30">d</script>-dimensional simplex and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-197" style="width: 2.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-198"><span class="mo" id="MathJax-Span-199" style="font-family: MathJax_Main;">∥</span><span class="mo" id="MathJax-Span-200" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="mo" id="MathJax-Span-201" style="font-family: MathJax_Main; padding-left: 0.211em;">∥</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">‖</mo><mo>⋅</mo><mo fence="false" stretchy="false">‖</mo></math></span></span><script type="math/tex" id="MathJax-Element-31">\Vert \cdot \Vert</script> is the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-202" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-203"><span class="msubsup" id="MathJax-Span-204"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-205" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mn" id="MathJax-Span-206" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-32">\ell_1</script>-norm, the existence of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-207" style="width: 6.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1005.26em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-208"><span class="mi" id="MathJax-Span-209" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-210" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-211"><span style="display: inline-block; position: relative; width: 3.805em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1002.82em, 2.503em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-212"><span class="mi" id="MathJax-Span-213" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mi" id="MathJax-Span-214" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-215"></span><span class="mi" id="MathJax-Span-216" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1002.76em, 3.961em, -999.997em); top: -4.581em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 2.086em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.471em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.044em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.565em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-217" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>T</mi><mi>log</mi><mo>⁡</mo><mi>d</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-33">O(\sqrt{T\log d})</script> algorithms for learning with experts implies that it is possible to obtain <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-218" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-219"><span class="mi" id="MathJax-Span-220" style="font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-34">\epsilon</script>-calibrated forecasts after <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-221" style="width: 16.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.544em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1013.54em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-222"><span class="mi" id="MathJax-Span-223" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-224" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mi" id="MathJax-Span-225" style="font-family: MathJax_Main; padding-left: 0.263em;">exp</span><span class="mo" id="MathJax-Span-226"></span><span class="mo" id="MathJax-Span-227" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-228" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-229" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-230" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-231"></span><span class="texatom" id="MathJax-Span-232" style="padding-left: 0.159em;"><span class="mrow" id="MathJax-Span-233"><span class="mi" id="MathJax-Span-234" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span class="texatom" id="MathJax-Span-235"><span class="mrow" id="MathJax-Span-236"><span class="mo" id="MathJax-Span-237" style="font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-238"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-239" style="font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="mn" id="MathJax-Span-240" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-241" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-242" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-243" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="msubsup" id="MathJax-Span-244" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-245" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-246"><span class="mrow" id="MathJax-Span-247"><span class="mi" id="MathJax-Span-248" style="font-size: 70.7%; font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-249" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-250" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-251"><span class="mrow" id="MathJax-Span-252"><span class="mo" id="MathJax-Span-253" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-254"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-255" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.315em;"><span class="mn" id="MathJax-Span-256" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-257" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>O</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-35">T = \exp(O(\log{d}/\epsilon^2)) = d^{O(1/\epsilon^2)}</script> rounds, recovering a recent result of Peng 2025. Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-258" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-259"><span class="mi" id="MathJax-Span-260" style="font-family: MathJax_Math-italic;">ρ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ρ</mi></math></span></span><script type="math/tex" id="MathJax-Element-36">\rho</script> -- in fact, our algorithm is identical for every setting of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-261" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-262"><span class="texatom" id="MathJax-Span-263"><span class="mrow" id="MathJax-Span-264"><span class="mi" id="MathJax-Span-265" style="font-family: MathJax_Caligraphic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">P</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-37">\mathcal{P}</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-266" style="width: 2.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-267"><span class="mo" id="MathJax-Span-268" style="font-family: MathJax_Main;">∥</span><span class="mo" id="MathJax-Span-269" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="mo" id="MathJax-Span-270" style="font-family: MathJax_Main; padding-left: 0.211em;">∥</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">‖</mo><mo>⋅</mo><mo fence="false" stretchy="false">‖</mo></math></span></span><script type="math/tex" id="MathJax-Element-38">\Vert \cdot \Vert</script>. Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round. Finally, we prove that any online calibration algorithm that guarantees <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-271" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-272"><span class="mi" id="MathJax-Span-273" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mi" id="MathJax-Span-274" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-39">\epsilon T</script> <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-275" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-276"><span class="msubsup" id="MathJax-Span-277"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-278" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mn" id="MathJax-Span-279" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-40">\ell_1</script>-calibration error over the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-280" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-281"><span class="mi" id="MathJax-Span-282" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-41">d</script>-dimensional simplex requires <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-283" style="width: 10.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.336em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1008.23em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-284"><span class="mi" id="MathJax-Span-285" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-286" style="font-family: MathJax_Main; padding-left: 0.263em;">≥</span><span class="mi" id="MathJax-Span-287" style="font-family: MathJax_Main; padding-left: 0.263em;">exp</span><span class="mo" id="MathJax-Span-288"></span><span class="mo" id="MathJax-Span-289" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-290"><span class="mrow" id="MathJax-Span-291"><span class="mi" id="MathJax-Span-292" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-293" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-294" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-295" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-296" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-297" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-298"><span class="mrow" id="MathJax-Span-299"><span class="mo" id="MathJax-Span-300" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-301" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-302" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-303" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mo>≥</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-42">T \geq \exp(\mathrm{poly}(1/\epsilon))</script> (assuming <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-304" style="width: 7.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.78em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-305"><span class="mi" id="MathJax-Span-306" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-307" style="font-family: MathJax_Main; padding-left: 0.263em;">≥</span><span class="texatom" id="MathJax-Span-308" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-309"><span class="mi" id="MathJax-Span-310" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-311" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-312" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-313" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-314" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-315" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-316"><span class="mrow" id="MathJax-Span-317"><span class="mo" id="MathJax-Span-318" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-319" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-320" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mo>≥</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-43">d \geq \mathrm{poly}(1/\epsilon)</script>). This strengthens the corresponding <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-321" style="width: 4.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.75em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-322"><span class="msubsup" id="MathJax-Span-323"><span style="display: inline-block; position: relative; width: 3.753em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-324" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-325"><span class="mrow" id="MathJax-Span-326"><span class="mi" id="MathJax-Span-327" style="font-size: 70.7%; font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-328" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-329" style="font-size: 70.7%; font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-330" style="font-size: 70.7%;"></span><span class="texatom" id="MathJax-Span-331" style="padding-left: 0.159em;"><span class="mrow" id="MathJax-Span-332"><span class="mn" id="MathJax-Span-333" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-334"><span class="mrow" id="MathJax-Span-335"><span class="mo" id="MathJax-Span-336" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-337" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span></span></span><span class="mo" id="MathJax-Span-338" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ϵ</mi></mrow><mo stretchy="false">)</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-44">d^{\Omega(\log{1/\epsilon})}</script> lower bound of Peng 2025, and shows that an exponential dependence on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-339" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.41em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-340"><span class="mn" id="MathJax-Span-341" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-342"><span class="mrow" id="MathJax-Span-343"><span class="mo" id="MathJax-Span-344" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-345" style="font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-45">1/\epsilon</script> is necessary.</p>
            <p id="subjects-UVDihUz0iT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-UVDihUz0iT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UVDihUz0iT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UVDihUz0iT@OpenReview" onclick="foldPdfKimi('UVDihUz0iT@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="CH72XyZs4y@OpenReview" class="panel paper" keywords="adam,sauce,signed,momentum,secret,insights,beta,underperform,reformulations,affords">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CH72XyZs4y" target="_blank" title="14/77"><span class="index notranslate">#14</span></a>
                <a id="title-CH72XyZs4y@OpenReview" class="title-link" href="/venue/CH72XyZs4y@OpenReview" target="_blank">In Search of Adam’s Secret Sauce</a>
                <a id="pdf-CH72XyZs4y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CH72XyZs4y@OpenReview', this)" data="https://openreview.net/pdf?id=CH72XyZs4y">[PDF<sup id="pdf-stars-CH72XyZs4y@OpenReview">15</sup>]</a>
                <a id="copy-CH72XyZs4y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CH72XyZs4y@OpenReview')">[Copy]</a>
                <a id="kimi-CH72XyZs4y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CH72XyZs4y@OpenReview', this)">[Kimi<sup id="kimi-stars-CH72XyZs4y@OpenReview">18</sup>]</a>
                <a id="rel-CH72XyZs4y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CH72XyZs4y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CH72XyZs4y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Antonio Orvieto" target="_blank">Antonio Orvieto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert M. Gower" target="_blank">Robert M. Gower</a>
            </p>
            <p id="summary-CH72XyZs4y@OpenReview" class="summary">Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-346" style="width: 3.961em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.28em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-347"><span class="msubsup" id="MathJax-Span-348"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-349" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.576em;"><span class="mn" id="MathJax-Span-350" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-351" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="msubsup" id="MathJax-Span-352" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-353" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.576em;"><span class="mn" id="MathJax-Span-354" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><msub><mi>β</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-46">\beta_1=\beta_2</script>. Beyond robust performance, this choice affords new theoretical insights, highlights the "secret sauce" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective.</p>
            <p id="subjects-CH72XyZs4y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-CH72XyZs4y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CH72XyZs4y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CH72XyZs4y@OpenReview" onclick="foldPdfKimi('CH72XyZs4y@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="U8BwT6Rmw4@OpenReview" class="panel paper" keywords="franz,parisi,statistical,criterion,ngca,lower,bounds,truncation,convex,equivalence">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U8BwT6Rmw4" target="_blank" title="15/77"><span class="index notranslate">#15</span></a>
                <a id="title-U8BwT6Rmw4@OpenReview" class="title-link" href="/venue/U8BwT6Rmw4@OpenReview" target="_blank">An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds</a>
                <a id="pdf-U8BwT6Rmw4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U8BwT6Rmw4@OpenReview', this)" data="https://openreview.net/pdf?id=U8BwT6Rmw4">[PDF<sup id="pdf-stars-U8BwT6Rmw4@OpenReview">8</sup>]</a>
                <a id="copy-U8BwT6Rmw4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U8BwT6Rmw4@OpenReview')">[Copy]</a>
                <a id="kimi-U8BwT6Rmw4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U8BwT6Rmw4@OpenReview', this)">[Kimi<sup id="kimi-stars-U8BwT6Rmw4@OpenReview">4</sup>]</a>
                <a id="rel-U8BwT6Rmw4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U8BwT6Rmw4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U8BwT6Rmw4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Chen" target="_blank">Siyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Theodor Misiakiewicz" target="_blank">Theodor Misiakiewicz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilias Zadik" target="_blank">Ilias Zadik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiyuan Zhang" target="_blank">Peiyuan Zhang</a>
            </p>
            <p id="summary-U8BwT6Rmw4@OpenReview" class="summary">Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds---another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry. In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds—such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024)—but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023).</p>
            <p id="subjects-U8BwT6Rmw4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-U8BwT6Rmw4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U8BwT6Rmw4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U8BwT6Rmw4@OpenReview" onclick="foldPdfKimi('U8BwT6Rmw4@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="efOq8wHH9o@OpenReview" class="panel paper" keywords="maxsup,overconfidence,logit,smoothing,regularization,zhouyuxuanyx,incorrect,collapse,label,predictions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=efOq8wHH9o" target="_blank" title="16/77"><span class="index notranslate">#16</span></a>
                <a id="title-efOq8wHH9o@OpenReview" class="title-link" href="/venue/efOq8wHH9o@OpenReview" target="_blank">MaxSup: Overcoming Representation Collapse in Label Smoothing</a>
                <a id="pdf-efOq8wHH9o@OpenReview" class="title-pdf notranslate" onclick="togglePdf('efOq8wHH9o@OpenReview', this)" data="https://openreview.net/pdf?id=efOq8wHH9o">[PDF<sup id="pdf-stars-efOq8wHH9o@OpenReview">18</sup>]</a>
                <a id="copy-efOq8wHH9o@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('efOq8wHH9o@OpenReview')">[Copy]</a>
                <a id="kimi-efOq8wHH9o@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('efOq8wHH9o@OpenReview', this)">[Kimi<sup id="kimi-stars-efOq8wHH9o@OpenReview">15</sup>]</a>
                <a id="rel-efOq8wHH9o@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('efOq8wHH9o@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-efOq8wHH9o@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Zhou" target="_blank">Yuxuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Li" target="_blank">Heng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi-Qi Cheng" target="_blank">Zhi-Qi Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xudong Yan" target="_blank">Xudong Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Dong" target="_blank">Yifei Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Fritz" target="_blank">Mario Fritz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margret Keuper" target="_blank">Margret Keuper</a>
            </p>
            <p id="summary-efOq8wHH9o@OpenReview" class="summary">Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS.Code and reproducibility scripts are available at https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization.</p>
            <p id="subjects-efOq8wHH9o@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-efOq8wHH9o@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-efOq8wHH9o@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-efOq8wHH9o@OpenReview" onclick="foldPdfKimi('efOq8wHH9o@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="IfD2MKTmWv@OpenReview" class="panel paper" keywords="mosaics,memory,trillion,scale,tokens,transformers,knowledge,capabilities,10b,storage">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IfD2MKTmWv" target="_blank" title="17/77"><span class="index notranslate">#17</span></a>
                <a id="title-IfD2MKTmWv@OpenReview" class="title-link" href="/venue/IfD2MKTmWv@OpenReview" target="_blank">Memory Mosaics at scale</a>
                <a id="pdf-IfD2MKTmWv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IfD2MKTmWv@OpenReview', this)" data="https://openreview.net/pdf?id=IfD2MKTmWv">[PDF<sup id="pdf-stars-IfD2MKTmWv@OpenReview">18</sup>]</a>
                <a id="copy-IfD2MKTmWv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IfD2MKTmWv@OpenReview')">[Copy]</a>
                <a id="kimi-IfD2MKTmWv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IfD2MKTmWv@OpenReview', this)">[Kimi<sup id="kimi-stars-IfD2MKTmWv@OpenReview">15</sup>]</a>
                <a id="rel-IfD2MKTmWv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IfD2MKTmWv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IfD2MKTmWv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyu Zhang" target="_blank">Jianyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leon Bottou" target="_blank">Leon Bottou</a>
            </p>
            <p id="summary-IfD2MKTmWv@OpenReview" class="summary">Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.</p>
            <p id="subjects-IfD2MKTmWv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-IfD2MKTmWv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IfD2MKTmWv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IfD2MKTmWv@OpenReview" onclick="foldPdfKimi('IfD2MKTmWv@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="jMhRbV47pS@OpenReview" class="panel paper" keywords="emergence,attention,sparse,repetition,abilities,transformers,understanding,task,benefits,fascinating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jMhRbV47pS" target="_blank" title="18/77"><span class="index notranslate">#18</span></a>
                <a id="title-jMhRbV47pS@OpenReview" class="title-link" href="/venue/jMhRbV47pS@OpenReview" target="_blank">The emergence of sparse attention: impact of data distribution and benefits of repetition</a>
                <a id="pdf-jMhRbV47pS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jMhRbV47pS@OpenReview', this)" data="https://openreview.net/pdf?id=jMhRbV47pS">[PDF<sup id="pdf-stars-jMhRbV47pS@OpenReview">21</sup>]</a>
                <a id="copy-jMhRbV47pS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jMhRbV47pS@OpenReview')">[Copy]</a>
                <a id="kimi-jMhRbV47pS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jMhRbV47pS@OpenReview', this)">[Kimi<sup id="kimi-stars-jMhRbV47pS@OpenReview">17</sup>]</a>
                <a id="rel-jMhRbV47pS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jMhRbV47pS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jMhRbV47pS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Zucchet" target="_blank">Nicolas Zucchet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco D'Angelo" target="_blank">Francesco D'Angelo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Kyle Lampinen" target="_blank">Andrew Kyle Lampinen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephanie C.Y. Chan" target="_blank">Stephanie C.Y. Chan</a>
            </p>
            <p id="summary-jMhRbV47pS@OpenReview" class="summary">Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.</p>
            <p id="subjects-jMhRbV47pS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-jMhRbV47pS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jMhRbV47pS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jMhRbV47pS@OpenReview" onclick="foldPdfKimi('jMhRbV47pS@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="aLhA7AYLLR@OpenReview" class="panel paper" keywords="degradations,degradation,controlfusion,fusion,prompts,composite,user,controllable,vision,network">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aLhA7AYLLR" target="_blank" title="19/77"><span class="index notranslate">#19</span></a>
                <a id="title-aLhA7AYLLR@OpenReview" class="title-link" href="/venue/aLhA7AYLLR@OpenReview" target="_blank">ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts</a>
                <a id="pdf-aLhA7AYLLR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aLhA7AYLLR@OpenReview', this)" data="https://openreview.net/pdf?id=aLhA7AYLLR">[PDF<sup id="pdf-stars-aLhA7AYLLR@OpenReview">20</sup>]</a>
                <a id="copy-aLhA7AYLLR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aLhA7AYLLR@OpenReview')">[Copy]</a>
                <a id="kimi-aLhA7AYLLR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aLhA7AYLLR@OpenReview', this)">[Kimi<sup id="kimi-stars-aLhA7AYLLR@OpenReview">12</sup>]</a>
                <a id="rel-aLhA7AYLLR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aLhA7AYLLR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aLhA7AYLLR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Tang" target="_blank">Linfeng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeda Wang" target="_blank">Yeda Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanchuan Cai" target="_blank">Zhanchuan Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjun Jiang" target="_blank">Junjun Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Ma" target="_blank">Jiayi Ma</a>
            </p>
            <p id="summary-aLhA7AYLLR@OpenReview" class="summary">Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations.</p>
            <p id="subjects-aLhA7AYLLR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-aLhA7AYLLR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aLhA7AYLLR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aLhA7AYLLR@OpenReview" onclick="foldPdfKimi('aLhA7AYLLR@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="MrUsZfQ9pC@OpenReview" class="panel paper" keywords="identifiability,pnns,activation,widths,identifiable,degrees,neurovarieties,decoder,pnn,deep">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MrUsZfQ9pC" target="_blank" title="20/77"><span class="index notranslate">#20</span></a>
                <a id="title-MrUsZfQ9pC@OpenReview" class="title-link" href="/venue/MrUsZfQ9pC@OpenReview" target="_blank">Identifiability of Deep Polynomial Neural Networks</a>
                <a id="pdf-MrUsZfQ9pC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MrUsZfQ9pC@OpenReview', this)" data="https://openreview.net/pdf?id=MrUsZfQ9pC">[PDF<sup id="pdf-stars-MrUsZfQ9pC@OpenReview">6</sup>]</a>
                <a id="copy-MrUsZfQ9pC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MrUsZfQ9pC@OpenReview')">[Copy]</a>
                <a id="kimi-MrUsZfQ9pC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MrUsZfQ9pC@OpenReview', this)">[Kimi<sup id="kimi-stars-MrUsZfQ9pC@OpenReview">8</sup>]</a>
                <a id="rel-MrUsZfQ9pC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MrUsZfQ9pC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MrUsZfQ9pC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Konstantin Usevich" target="_blank">Konstantin Usevich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricardo Augusto Borsoi" target="_blank">Ricardo Augusto Borsoi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clara Dérand" target="_blank">Clara Dérand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marianne Clausel" target="_blank">Marianne Clausel</a>
            </p>
            <p id="summary-MrUsZfQ9pC@OpenReview" class="summary">Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability-a key property for ensuring interpretability-remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension.</p>
            <p id="subjects-MrUsZfQ9pC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-MrUsZfQ9pC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MrUsZfQ9pC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MrUsZfQ9pC@OpenReview" onclick="foldPdfKimi('MrUsZfQ9pC@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Q3qAsZAEZw@OpenReview" class="panel paper" keywords="precision,reproducibility,llm,gpu,nondeterminism,differences,inference,floating,numerical,evaluation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Q3qAsZAEZw" target="_blank" title="21/77"><span class="index notranslate">#21</span></a>
                <a id="title-Q3qAsZAEZw@OpenReview" class="title-link" href="/venue/Q3qAsZAEZw@OpenReview" target="_blank">Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference</a>
                <a id="pdf-Q3qAsZAEZw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Q3qAsZAEZw@OpenReview', this)" data="https://openreview.net/pdf?id=Q3qAsZAEZw">[PDF<sup id="pdf-stars-Q3qAsZAEZw@OpenReview">19</sup>]</a>
                <a id="copy-Q3qAsZAEZw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Q3qAsZAEZw@OpenReview')">[Copy]</a>
                <a id="kimi-Q3qAsZAEZw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Q3qAsZAEZw@OpenReview', this)">[Kimi<sup id="kimi-stars-Q3qAsZAEZw@OpenReview">16</sup>]</a>
                <a id="rel-Q3qAsZAEZw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Q3qAsZAEZw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Q3qAsZAEZw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Yuan" target="_blank">Jiayi Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinheng Ding" target="_blank">Xinheng Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenya Xie" target="_blank">Wenya Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Jhe Li" target="_blank">Yu-Jhe Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentian Zhao" target="_blank">Wentian Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wan" target="_blank">Kun Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Shi" target="_blank">Jing Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xia Hu" target="_blank">Xia Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zirui Liu" target="_blank">Zirui Liu</a>
            </p>
            <p id="summary-Q3qAsZAEZw@OpenReview" class="summary">Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.</p>
            <p id="subjects-Q3qAsZAEZw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-Q3qAsZAEZw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Q3qAsZAEZw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Q3qAsZAEZw@OpenReview" onclick="foldPdfKimi('Q3qAsZAEZw@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="4xvE6Iy77Y@OpenReview" class="panel paper" keywords="primt,trajectory,pbrl,feedback,preference,multimodal,foundation,ambiguity,credit,reinforcement">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4xvE6Iy77Y" target="_blank" title="22/77"><span class="index notranslate">#22</span></a>
                <a id="title-4xvE6Iy77Y@OpenReview" class="title-link" href="/venue/4xvE6Iy77Y@OpenReview" target="_blank">PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</a>
                <a id="pdf-4xvE6Iy77Y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4xvE6Iy77Y@OpenReview', this)" data="https://openreview.net/pdf?id=4xvE6Iy77Y">[PDF<sup id="pdf-stars-4xvE6Iy77Y@OpenReview">12</sup>]</a>
                <a id="copy-4xvE6Iy77Y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4xvE6Iy77Y@OpenReview')">[Copy]</a>
                <a id="kimi-4xvE6Iy77Y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4xvE6Iy77Y@OpenReview', this)">[Kimi<sup id="kimi-stars-4xvE6Iy77Y@OpenReview">14</sup>]</a>
                <a id="rel-4xvE6Iy77Y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4xvE6Iy77Y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4xvE6Iy77Y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqi Wang" target="_blank">Ruiqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dezhong Zhao" target="_blank">Dezhong Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqin Yuan" target="_blank">Ziqin Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Shao" target="_blank">Tianyu Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guohua Chen" target="_blank">Guohua Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dominic Kao" target="_blank">Dominic Kao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungeun Hong" target="_blank">Sungeun Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Byung-Cheol Min" target="_blank">Byung-Cheol Min</a>
            </p>
            <p id="summary-4xvE6Iy77Y@OpenReview" class="summary">Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/.</p>
            <p id="subjects-4xvE6Iy77Y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-4xvE6Iy77Y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4xvE6Iy77Y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4xvE6Iy77Y@OpenReview" onclick="foldPdfKimi('4xvE6Iy77Y@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="R73ybUciQF@OpenReview" class="panel paper" keywords="saes,absorption,features,sae,feature,splitting,sparse,autoencoders,monosemantic,split">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=R73ybUciQF" target="_blank" title="23/77"><span class="index notranslate">#23</span></a>
                <a id="title-R73ybUciQF@OpenReview" class="title-link" href="/venue/R73ybUciQF@OpenReview" target="_blank">A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</a>
                <a id="pdf-R73ybUciQF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('R73ybUciQF@OpenReview', this)" data="https://openreview.net/pdf?id=R73ybUciQF">[PDF<sup id="pdf-stars-R73ybUciQF@OpenReview">10</sup>]</a>
                <a id="copy-R73ybUciQF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('R73ybUciQF@OpenReview')">[Copy]</a>
                <a id="kimi-R73ybUciQF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('R73ybUciQF@OpenReview', this)">[Kimi<sup id="kimi-stars-R73ybUciQF@OpenReview">14</sup>]</a>
                <a id="rel-R73ybUciQF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('R73ybUciQF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-R73ybUciQF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David Chanin" target="_blank">David Chanin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Wilken-Smith" target="_blank">James Wilken-Smith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomáš Dulka" target="_blank">Tomáš Dulka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hardik Bhatnagar" target="_blank">Hardik Bhatnagar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Satvik Golechha" target="_blank">Satvik Golechha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joseph Isaac Bloom" target="_blank">Joseph Isaac Bloom</a>
            </p>
            <p id="summary-R73ybUciQF@OpenReview" class="summary">Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.</p>
            <p id="subjects-R73ybUciQF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-R73ybUciQF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-R73ybUciQF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-R73ybUciQF@OpenReview" onclick="foldPdfKimi('R73ybUciQF@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="B6bE2GC71a@OpenReview" class="panel paper" keywords="training,pre,evolm,continued,post,language,lms,stages,domain,lost">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=B6bE2GC71a" target="_blank" title="24/77"><span class="index notranslate">#24</span></a>
                <a id="title-B6bE2GC71a@OpenReview" class="title-link" href="/venue/B6bE2GC71a@OpenReview" target="_blank">EvoLM: In Search of Lost Language Model Training Dynamics</a>
                <a id="pdf-B6bE2GC71a@OpenReview" class="title-pdf notranslate" onclick="togglePdf('B6bE2GC71a@OpenReview', this)" data="https://openreview.net/pdf?id=B6bE2GC71a">[PDF<sup id="pdf-stars-B6bE2GC71a@OpenReview">14</sup>]</a>
                <a id="copy-B6bE2GC71a@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('B6bE2GC71a@OpenReview')">[Copy]</a>
                <a id="kimi-B6bE2GC71a@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('B6bE2GC71a@OpenReview', this)">[Kimi<sup id="kimi-stars-B6bE2GC71a@OpenReview">18</sup>]</a>
                <a id="rel-B6bE2GC71a@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('B6bE2GC71a@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-B6bE2GC71a@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenting Qi" target="_blank">Zhenting Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Nie" target="_blank">Fan Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Alahi" target="_blank">Alexandre Alahi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Zou" target="_blank">James Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Himabindu Lakkaraju" target="_blank">Himabindu Lakkaraju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilun Du" target="_blank">Yilun Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric P. Xing" target="_blank">Eric P. Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sham M. Kakade" target="_blank">Sham M. Kakade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanlin Zhang" target="_blank">Hanlin Zhang</a>
            </p>
            <p id="summary-B6bE2GC71a@OpenReview" class="summary">Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.</p>
            <p id="subjects-B6bE2GC71a@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-B6bE2GC71a@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-B6bE2GC71a@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-B6bE2GC71a@OpenReview" onclick="foldPdfKimi('B6bE2GC71a@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="WhEPg4mUs6@OpenReview" class="panel paper" keywords="aimc,response,ideal,analog,training,resistive,functions,hardware,impact,conductance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WhEPg4mUs6" target="_blank" title="25/77"><span class="index notranslate">#25</span></a>
                <a id="title-WhEPg4mUs6@OpenReview" class="title-link" href="/venue/WhEPg4mUs6@OpenReview" target="_blank">Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</a>
                <a id="pdf-WhEPg4mUs6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WhEPg4mUs6@OpenReview', this)" data="https://openreview.net/pdf?id=WhEPg4mUs6">[PDF<sup id="pdf-stars-WhEPg4mUs6@OpenReview">8</sup>]</a>
                <a id="copy-WhEPg4mUs6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WhEPg4mUs6@OpenReview')">[Copy]</a>
                <a id="kimi-WhEPg4mUs6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WhEPg4mUs6@OpenReview', this)">[Kimi<sup id="kimi-stars-WhEPg4mUs6@OpenReview">11</sup>]</a>
                <a id="rel-WhEPg4mUs6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WhEPg4mUs6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WhEPg4mUs6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxian Wu" target="_blank">Zhaoxian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Xiao" target="_blank">Quan Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tayfun Gokmen" target="_blank">Tayfun Gokmen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omobayode Fagbohungbe" target="_blank">Omobayode Fagbohungbe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Chen" target="_blank">Tianyi Chen</a>
            </p>
            <p id="summary-WhEPg4mUs6@OpenReview" class="summary">As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.</p>
            <p id="subjects-WhEPg4mUs6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-WhEPg4mUs6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WhEPg4mUs6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WhEPg4mUs6@OpenReview" onclick="foldPdfKimi('WhEPg4mUs6@OpenReview', this)" class="hr hr-fold">
        </div>
    <div id="zJdutIT6vT@OpenReview" class="panel paper" keywords="conflicts,opinion,signed,intervals,graphs,opinions,signs,german,interactions,discovering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zJdutIT6vT" target="_blank" title="26/77"><span class="index notranslate">#26</span></a>
                <a id="title-zJdutIT6vT@OpenReview" class="title-link" href="/venue/zJdutIT6vT@OpenReview" target="_blank">Discovering Opinion Intervals from Conflicts in Signed Graphs</a>
                <a id="pdf-zJdutIT6vT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zJdutIT6vT@OpenReview', this)" data="https://openreview.net/pdf?id=zJdutIT6vT">[PDF<sup id="pdf-stars-zJdutIT6vT@OpenReview">7</sup>]</a>
                <a id="copy-zJdutIT6vT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zJdutIT6vT@OpenReview')">[Copy]</a>
                <a id="kimi-zJdutIT6vT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zJdutIT6vT@OpenReview', this)">[Kimi<sup id="kimi-stars-zJdutIT6vT@OpenReview">8</sup>]</a>
                <a id="rel-zJdutIT6vT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zJdutIT6vT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zJdutIT6vT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Blohm" target="_blank">Peter Blohm</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Chen" target="_blank">Florian Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aristides Gionis" target="_blank">Aristides Gionis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Neumann" target="_blank">Stefan Neumann</a>
            </p>
            <p id="summary-zJdutIT6vT@OpenReview" class="summary">Online social media provide a platform for people to discuss current events and exchange opinions with their peers. While interactions are predominantly positive, in recent years, there has been a lot of research to understand the conflicts in social networks and how they are based on different views and opinions. In this paper, we ask whether the conflicts in a network reveal a small and interpretable set of prevalent opinion ranges that explain the users' interactions. More precisely, we consider signed graphs, where the edge signs indicate positive and negative interactions of node pairs, and our goal is to infer opinion intervals that are consistent with the edge signs. We introduce an optimization problem that models this question, and we give strong hardness results and a polynomial-time approximation scheme by utilizing connections to interval graphs and the Correlation Clustering problem. We further provide scalable heuristics and show that in experiments they yield more expressive solutions than Correlation Clustering baselines. We also present a case study on a novel real-world dataset from the German parliament, showing that our algorithms can recover the political leaning of German parties based on co-voting behavior.</p>
            <p id="subjects-zJdutIT6vT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-zJdutIT6vT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zJdutIT6vT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zJdutIT6vT@OpenReview" onclick="foldPdfKimi('zJdutIT6vT@OpenReview', this)" class="hr hr-fold">
        </div><div id="8P3QNSckMp@OpenReview" class="panel paper" keywords="unifloral,offline,implementations,algorithmic,clean,slate,hyperparameter,awr,reinforcement,evaluation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8P3QNSckMp" target="_blank" title="27/77"><span class="index notranslate">#27</span></a>
                <a id="title-8P3QNSckMp@OpenReview" class="title-link" href="/venue/8P3QNSckMp@OpenReview" target="_blank">A Clean Slate for Offline Reinforcement Learning</a>
                <a id="pdf-8P3QNSckMp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8P3QNSckMp@OpenReview', this)" data="https://openreview.net/pdf?id=8P3QNSckMp">[PDF<sup id="pdf-stars-8P3QNSckMp@OpenReview">13</sup>]</a>
                <a id="copy-8P3QNSckMp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8P3QNSckMp@OpenReview')">[Copy]</a>
                <a id="kimi-8P3QNSckMp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8P3QNSckMp@OpenReview', this)">[Kimi<sup id="kimi-stars-8P3QNSckMp@OpenReview">11</sup>]</a>
                <a id="rel-8P3QNSckMp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8P3QNSckMp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8P3QNSckMp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Thomas Jackson" target="_blank">Matthew Thomas Jackson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Uljad Berdica" target="_blank">Uljad Berdica</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jarek Luca Liesen" target="_blank">Jarek Luca Liesen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shimon Whiteson" target="_blank">Shimon Whiteson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Nicolaus Foerster" target="_blank">Jakob Nicolaus Foerster</a>
            </p>
            <p id="summary-8P3QNSckMp@OpenReview" class="summary">Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral.</p>
            <p id="subjects-8P3QNSckMp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-8P3QNSckMp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8P3QNSckMp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8P3QNSckMp@OpenReview" onclick="foldPdfKimi('8P3QNSckMp@OpenReview', this)" class="hr hr-fold">
        </div><div id="F0JzotXYgC@OpenReview" class="panel paper" keywords="spectral,norm,perturbation,rank,bounds,approximation,differentially,privacy,private,matrix">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=F0JzotXYgC" target="_blank" title="28/77"><span class="index notranslate">#28</span></a>
                <a id="title-F0JzotXYgC@OpenReview" class="title-link" href="/venue/F0JzotXYgC@OpenReview" target="_blank">Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy</a>
                <a id="pdf-F0JzotXYgC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('F0JzotXYgC@OpenReview', this)" data="https://openreview.net/pdf?id=F0JzotXYgC">[PDF<sup id="pdf-stars-F0JzotXYgC@OpenReview">5</sup>]</a>
                <a id="copy-F0JzotXYgC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('F0JzotXYgC@OpenReview')">[Copy]</a>
                <a id="kimi-F0JzotXYgC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('F0JzotXYgC@OpenReview', this)">[Kimi<sup id="kimi-stars-F0JzotXYgC@OpenReview">5</sup>]</a>
                <a id="rel-F0JzotXYgC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('F0JzotXYgC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-F0JzotXYgC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Phuc Tran" target="_blank">Phuc Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Van Vu" target="_blank">Van Vu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nisheeth K. Vishnoi" target="_blank">Nisheeth K. Vishnoi</a>
            </p>
            <p id="summary-F0JzotXYgC@OpenReview" class="summary">A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-355" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-356"><span class="mi" id="MathJax-Span-357" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-47">p</script> structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-358" style="width: 4.951em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1004.12em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-359"><span class="mi" id="MathJax-Span-360" style="font-family: MathJax_Math-italic;">A</span><span class="mo" id="MathJax-Span-361" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="msubsup" id="MathJax-Span-362" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.19em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.607em, -999.997em); top: -2.445em; left: 0em;"><span class="texatom" id="MathJax-Span-363"><span class="mrow" id="MathJax-Span-364"><span class="mi" id="MathJax-Span-365" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.732em;"><span class="texatom" id="MathJax-Span-366"><span class="mrow" id="MathJax-Span-367"><span class="mi" id="MathJax-Span-368" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-369" style="font-size: 70.7%; font-family: MathJax_Main;">×</span><span class="mi" id="MathJax-Span-370" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-48">A \in \mathbb{R}^{n \times n}</script> and an arbitrary symmetric perturbation <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-371" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-372"><span class="mi" id="MathJax-Span-373" style="font-family: MathJax_Math-italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi></math></span></span><script type="math/tex" id="MathJax-Element-49">E</script>. Under mild eigengap and norm conditions, our bounds yield sharp estimates for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-374" style="width: 8.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1007.09em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-375"><span class="mo" id="MathJax-Span-376" style="font-family: MathJax_Main;">∥</span><span class="mo" id="MathJax-Span-377" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-378" style="font-family: MathJax_Math-italic;">A</span><span class="mo" id="MathJax-Span-379" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-380" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msubsup" id="MathJax-Span-381"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.26em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-382" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.367em;"><span class="mi" id="MathJax-Span-383" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-384" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="msubsup" id="MathJax-Span-385" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-386" style="font-family: MathJax_Math-italic;">A</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-387" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-388" style="font-family: MathJax_Main;">∥</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">‖</mo><mo stretchy="false">(</mo><mi>A</mi><mo>+</mo><mi>E</mi><msub><mo stretchy="false">)</mo><mi>p</mi></msub><mo>−</mo><msub><mi>A</mi><mi>p</mi></msub><mo fence="false" stretchy="false">‖</mo></math></span></span><script type="math/tex" id="MathJax-Element-50">\| (A + E)_p - A_p \|</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-389" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-390"><span class="msubsup" id="MathJax-Span-391"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-392" style="font-family: MathJax_Math-italic;">A</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-393" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>A</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-51">A_p</script> is the best rank-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-394" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-395"><span class="mi" id="MathJax-Span-396" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-52">p</script> approximation of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-397" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-398"><span class="mi" id="MathJax-Span-399" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span><script type="math/tex" id="MathJax-Element-53">A</script>, with improvements of up to a factor of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-54-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-400" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-401"><span class="msqrt" id="MathJax-Span-402"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-403"><span class="mi" id="MathJax-Span-404" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></span></span><script type="math/tex" id="MathJax-Element-54">\sqrt{n}</script>. As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes.</p>
            <p id="subjects-F0JzotXYgC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-F0JzotXYgC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-F0JzotXYgC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-F0JzotXYgC@OpenReview" onclick="foldPdfKimi('F0JzotXYgC@OpenReview', this)" class="hr hr-fold">
        </div><div id="gxfusMqPIs@OpenReview" class="panel paper" keywords="regret,ucb,gaussian,upper,bound,bayesian,process,scarlett,confidence,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gxfusMqPIs" target="_blank" title="29/77"><span class="index notranslate">#29</span></a>
                <a id="title-gxfusMqPIs@OpenReview" class="title-link" href="/venue/gxfusMqPIs@OpenReview" target="_blank">Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization</a>
                <a id="pdf-gxfusMqPIs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gxfusMqPIs@OpenReview', this)" data="https://openreview.net/pdf?id=gxfusMqPIs">[PDF<sup id="pdf-stars-gxfusMqPIs@OpenReview">5</sup>]</a>
                <a id="copy-gxfusMqPIs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gxfusMqPIs@OpenReview')">[Copy]</a>
                <a id="kimi-gxfusMqPIs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gxfusMqPIs@OpenReview', this)">[Kimi<sup id="kimi-stars-gxfusMqPIs@OpenReview">5</sup>]</a>
                <a id="rel-gxfusMqPIs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gxfusMqPIs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gxfusMqPIs@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shogo Iwazaki" target="_blank">Shogo Iwazaki</a>
            </p>
            <p id="summary-gxfusMqPIs@OpenReview" class="summary">This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Matérn kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-55-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-405" style="width: 3.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1002.97em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-406"><span class="texatom" id="MathJax-Span-407"><span class="mrow" id="MathJax-Span-408"><span class="munderover" id="MathJax-Span-409"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-410" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-411" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-412" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-413"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-414"><span class="mi" id="MathJax-Span-415" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-416" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-55">\tilde{O}(\sqrt{T})</script> cumulative regret with high probability. Furthermore, our analysis yields <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-56-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-417" style="width: 6.669em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.94em, 1005.42em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-418"><span class="mi" id="MathJax-Span-419" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-420" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-421"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px;"><span style="position: absolute; clip: rect(1.096em, 1002.97em, 2.294em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-422"><span class="mi" id="MathJax-Span-423" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="msubsup" id="MathJax-Span-424" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.84em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-425" style="font-family: MathJax_Main;">ln</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.836em;"><span class="mn" id="MathJax-Span-426" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-427"></span><span class="mi" id="MathJax-Span-428" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1002.92em, 3.961em, -999.997em); top: -4.789em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 2.242em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.367em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.836em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.357em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.826em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.216em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-429" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>T</mi><msup><mi>ln</mi><mn>2</mn></msup><mo>⁡</mo><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-56">O(\sqrt{T \ln^2 T})</script> regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner.</p>
            <p id="subjects-gxfusMqPIs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-gxfusMqPIs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gxfusMqPIs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gxfusMqPIs@OpenReview" onclick="foldPdfKimi('gxfusMqPIs@OpenReview', this)" class="hr hr-fold">
        </div><div id="eIDa6pd9iQ@OpenReview" class="panel paper" keywords="acns,auto,architectural,compressing,residual,connections,compression,redundancy,catastrophic,forgetting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eIDa6pd9iQ" target="_blank" title="30/77"><span class="index notranslate">#30</span></a>
                <a id="title-eIDa6pd9iQ@OpenReview" class="title-link" href="/venue/eIDa6pd9iQ@OpenReview" target="_blank">Auto-Compressing Networks</a>
                <a id="pdf-eIDa6pd9iQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eIDa6pd9iQ@OpenReview', this)" data="https://openreview.net/pdf?id=eIDa6pd9iQ">[PDF<sup id="pdf-stars-eIDa6pd9iQ@OpenReview">18</sup>]</a>
                <a id="copy-eIDa6pd9iQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eIDa6pd9iQ@OpenReview')">[Copy]</a>
                <a id="kimi-eIDa6pd9iQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eIDa6pd9iQ@OpenReview', this)">[Kimi<sup id="kimi-stars-eIDa6pd9iQ@OpenReview">14</sup>]</a>
                <a id="rel-eIDa6pd9iQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eIDa6pd9iQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eIDa6pd9iQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vaggelis Dorovatas" target="_blank">Vaggelis Dorovatas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Paraskevopoulos" target="_blank">Georgios Paraskevopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandros Potamianos" target="_blank">Alexandros Potamianos</a>
            </p>
            <p id="summary-eIDa6pd9iQ@OpenReview" class="summary">Deep neural networks with short residual connections have demonstrated remarkable success across domains, but increasing depth often introduces computational redundancy without corresponding improvements in representation quality. We introduce Auto-Compressing Networks (ACNs), an architectural variant where additive long feedforward connections from each layer to the output replace traditional short residual connections. By analyzing the distinct dynamics induced by this modification, we reveal a unique property we coin as *auto-compression*—the ability of a network to organically compress information during training with gradient descent, through architectural design alone. Through auto-compression, information is dynamically "pushed" into early layers during training, enhancing their representational quality and revealing potential redundancy in deeper ones. We theoretically show that this property emerges from layer-wise training patterns found only in ACNs, where layers are dynamically utilized during training based on task requirements. We also find that ACNs exhibit enhanced noise robustness compared to residual networks, superior performance in low-data settings, improved transfer learning capabilities, and mitigate catastrophic forgetting suggesting that they learn representations that generalize better despite using fewer parameters. Our results demonstrate up to 18\% reduction in catastrophic forgetting and 30-80\% architectural compression while maintaining accuracy across vision transformers, MLP-mixers, and BERT architectures. These findings establish ACNs as a practical approach to developing efficient neural architectures that automatically adapt their computational footprint to task complexity, while learning robust representations suitable for noisy real-world tasks and continual learning scenarios.</p>
            <p id="subjects-eIDa6pd9iQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-eIDa6pd9iQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eIDa6pd9iQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eIDa6pd9iQ@OpenReview" onclick="foldPdfKimi('eIDa6pd9iQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="oJ84bedrtM@OpenReview" class="panel paper" keywords="multimodal,moka,adaptation,mllms,unimodal,qwen2,tuning,modal,rank,fine">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oJ84bedrtM" target="_blank" title="31/77"><span class="index notranslate">#31</span></a>
                <a id="title-oJ84bedrtM@OpenReview" class="title-link" href="/venue/oJ84bedrtM@OpenReview" target="_blank">MokA: Multimodal Low-Rank Adaptation for MLLMs</a>
                <a id="pdf-oJ84bedrtM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oJ84bedrtM@OpenReview', this)" data="https://openreview.net/pdf?id=oJ84bedrtM">[PDF<sup id="pdf-stars-oJ84bedrtM@OpenReview">36</sup>]</a>
                <a id="copy-oJ84bedrtM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oJ84bedrtM@OpenReview')">[Copy]</a>
                <a id="kimi-oJ84bedrtM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oJ84bedrtM@OpenReview', this)">[Kimi<sup id="kimi-stars-oJ84bedrtM@OpenReview">22</sup>]</a>
                <a id="rel-oJ84bedrtM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oJ84bedrtM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oJ84bedrtM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yake Wei" target="_blank">Yake Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Miao" target="_blank">Yu Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongzhan Zhou" target="_blank">Dongzhan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Hu" target="_blank">Di Hu</a>
            </p>
            <p id="summary-oJ84bedrtM@OpenReview" class="summary">In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal Low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration.</p>
            <p id="subjects-oJ84bedrtM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-oJ84bedrtM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oJ84bedrtM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oJ84bedrtM@OpenReview" onclick="foldPdfKimi('oJ84bedrtM@OpenReview', this)" class="hr hr-fold">
        </div><div id="iydmH9boLb@OpenReview" class="panel paper" keywords="moe,specialization,expert,loss,experts,auxiliary,encourage,load,balancing,routing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=iydmH9boLb" target="_blank" title="32/77"><span class="index notranslate">#32</span></a>
                <a id="title-iydmH9boLb@OpenReview" class="title-link" href="/venue/iydmH9boLb@OpenReview" target="_blank">Advancing Expert Specialization for Better MoE</a>
                <a id="pdf-iydmH9boLb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('iydmH9boLb@OpenReview', this)" data="https://openreview.net/pdf?id=iydmH9boLb">[PDF<sup id="pdf-stars-iydmH9boLb@OpenReview">20</sup>]</a>
                <a id="copy-iydmH9boLb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('iydmH9boLb@OpenReview')">[Copy]</a>
                <a id="kimi-iydmH9boLb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('iydmH9boLb@OpenReview', this)">[Kimi<sup id="kimi-stars-iydmH9boLb@OpenReview">14</sup>]</a>
                <a id="rel-iydmH9boLb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('iydmH9boLb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-iydmH9boLb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongcan Guo" target="_blank">Hongcan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolang Lu" target="_blank">Haolang Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoshun Nan" target="_blank">Guoshun Nan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bolun Chu" target="_blank">Bolun Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jialin Zhuang" target="_blank">Jialin Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Yang" target="_blank">Yuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Che" target="_blank">Wenhao Che</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinye Cao" target="_blank">Xinye Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicong Leng" target="_blank">Sicong Leng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qimei Cui" target="_blank">Qimei Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xudong Jiang" target="_blank">Xudong Jiang</a>
            </p>
            <p id="summary-iydmH9boLb@OpenReview" class="summary">Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.</p>
            <p id="subjects-iydmH9boLb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-iydmH9boLb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-iydmH9boLb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-iydmH9boLb@OpenReview" onclick="foldPdfKimi('iydmH9boLb@OpenReview', this)" class="hr hr-fold">
        </div><div id="gm5mkiTGOy@OpenReview" class="panel paper" keywords="stage,initialization,matrices,transformer,dynamics,condensation,training,rank,collapse,dissects">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gm5mkiTGOy" target="_blank" title="33/77"><span class="index notranslate">#33</span></a>
                <a id="title-gm5mkiTGOy@OpenReview" class="title-link" href="/venue/gm5mkiTGOy@OpenReview" target="_blank">From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics</a>
                <a id="pdf-gm5mkiTGOy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gm5mkiTGOy@OpenReview', this)" data="https://openreview.net/pdf?id=gm5mkiTGOy">[PDF<sup id="pdf-stars-gm5mkiTGOy@OpenReview">9</sup>]</a>
                <a id="copy-gm5mkiTGOy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gm5mkiTGOy@OpenReview')">[Copy]</a>
                <a id="kimi-gm5mkiTGOy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gm5mkiTGOy@OpenReview', this)">[Kimi<sup id="kimi-stars-gm5mkiTGOy@OpenReview">6</sup>]</a>
                <a id="rel-gm5mkiTGOy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gm5mkiTGOy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gm5mkiTGOy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng-An Chen" target="_blank">Zheng-An Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Luo" target="_blank">Tao Luo</a>
            </p>
            <p id="summary-gm5mkiTGOy@OpenReview" class="summary">Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in \cite{zhou2022towards} to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results.</p>
            <p id="subjects-gm5mkiTGOy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-gm5mkiTGOy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gm5mkiTGOy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gm5mkiTGOy@OpenReview" onclick="foldPdfKimi('gm5mkiTGOy@OpenReview', this)" class="hr hr-fold">
        </div><div id="KnqiC0znVF@OpenReview" class="panel paper" keywords="llada,sft,language,diffusion,arms,gsai,reversal,capabilities,llms,llama3">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KnqiC0znVF" target="_blank" title="34/77"><span class="index notranslate">#34</span></a>
                <a id="title-KnqiC0znVF@OpenReview" class="title-link" href="/venue/KnqiC0znVF@OpenReview" target="_blank">Large Language Diffusion Models</a>
                <a id="pdf-KnqiC0znVF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KnqiC0znVF@OpenReview', this)" data="https://openreview.net/pdf?id=KnqiC0znVF">[PDF<sup id="pdf-stars-KnqiC0znVF@OpenReview">35</sup>]</a>
                <a id="copy-KnqiC0znVF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KnqiC0znVF@OpenReview')">[Copy]</a>
                <a id="kimi-KnqiC0znVF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KnqiC0znVF@OpenReview', this)">[Kimi<sup id="kimi-stars-KnqiC0znVF@OpenReview">19</sup>]</a>
                <a id="rel-KnqiC0znVF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KnqiC0znVF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KnqiC0znVF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shen Nie" target="_blank">Shen Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengqi Zhu" target="_blank">Fengqi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zebin You" target="_blank">Zebin You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolu Zhang" target="_blank">Xiaolu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyang Ou" target="_blank">Jingyang Ou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Hu" target="_blank">Jun Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=JUN ZHOU" target="_blank">JUN ZHOU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yankai Lin" target="_blank">Yankai Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji-Rong Wen" target="_blank">Ji-Rong Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chongxuan Li" target="_blank">Chongxuan Li</a>
            </p>
            <p id="summary-KnqiC0znVF@OpenReview" class="summary">The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \url{https://ml-gsai.github.io/LLaDA-demo/}.</p>
            <p id="subjects-KnqiC0znVF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-KnqiC0znVF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KnqiC0znVF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KnqiC0znVF@OpenReview" onclick="foldPdfKimi('KnqiC0znVF@OpenReview', this)" class="hr hr-fold">
        </div><div id="qYkhCah8OZ@OpenReview" class="panel paper" keywords="alfar,knowledge,mllms,logits,reallocation,contextual,attention,multimodal,tokens,conflicts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qYkhCah8OZ" target="_blank" title="35/77"><span class="index notranslate">#35</span></a>
                <a id="title-qYkhCah8OZ@OpenReview" class="title-link" href="/venue/qYkhCah8OZ@OpenReview" target="_blank">Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation</a>
                <a id="pdf-qYkhCah8OZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qYkhCah8OZ@OpenReview', this)" data="https://openreview.net/pdf?id=qYkhCah8OZ">[PDF<sup id="pdf-stars-qYkhCah8OZ@OpenReview">16</sup>]</a>
                <a id="copy-qYkhCah8OZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qYkhCah8OZ@OpenReview')">[Copy]</a>
                <a id="kimi-qYkhCah8OZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qYkhCah8OZ@OpenReview', this)">[Kimi<sup id="kimi-stars-qYkhCah8OZ@OpenReview">15</sup>]</a>
                <a id="rel-qYkhCah8OZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qYkhCah8OZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qYkhCah8OZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbin An" target="_blank">Wenbin An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Nie" target="_blank">Jiahao Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Tian" target="_blank">Feng Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haonan Lin" target="_blank">Haonan Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=mingxiang cai" target="_blank">mingxiang cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaqiang Wu" target="_blank">Yaqiang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=QianYing Wang" target="_blank">QianYing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqin Zhang" target="_blank">Xiaoqin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijian Lu" target="_blank">Shijian Lu</a>
            </p>
            <p id="summary-qYkhCah8OZ@OpenReview" class="summary">Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. However, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR.</p>
            <p id="subjects-qYkhCah8OZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-qYkhCah8OZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qYkhCah8OZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qYkhCah8OZ@OpenReview" onclick="foldPdfKimi('qYkhCah8OZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="fohuurA03P@OpenReview" class="panel paper" keywords="scene,interactive,answerer,retrieval,text,iat,t3sr,irr,scenes,retriever">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fohuurA03P" target="_blank" title="36/77"><span class="index notranslate">#36</span></a>
                <a id="title-fohuurA03P@OpenReview" class="title-link" href="/venue/fohuurA03P@OpenReview" target="_blank">Interactive Cross-modal Learning for Text-3D Scene Retrieval</a>
                <a id="pdf-fohuurA03P@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fohuurA03P@OpenReview', this)" data="https://openreview.net/pdf?id=fohuurA03P">[PDF<sup id="pdf-stars-fohuurA03P@OpenReview">13</sup>]</a>
                <a id="copy-fohuurA03P@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fohuurA03P@OpenReview')">[Copy]</a>
                <a id="kimi-fohuurA03P@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fohuurA03P@OpenReview', this)">[Kimi<sup id="kimi-stars-fohuurA03P@OpenReview">5</sup>]</a>
                <a id="rel-fohuurA03P@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fohuurA03P@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fohuurA03P@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yanglin Feng" target="_blank">Yanglin Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxiang Li" target="_blank">Yongxiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Sun" target="_blank">Yuan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Qin" target="_blank">Yang Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dezhong Peng" target="_blank">Dezhong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Hu" target="_blank">Peng Hu</a>
            </p>
            <p id="summary-fohuurA03P@OpenReview" class="summary">Text-3D Scene Retrieval (T3SR) aims to retrieve relevant scenes using linguistic queries. Although traditional T3SR methods have made significant progress in capturing fine-grained associations, they implicitly assume that query descriptions are information-complete. In practical deployments, however, limited by the capabilities of users and models, it is difficult or even impossible to directly obtain a perfect textual query suiting the entire scene and model, thereby leading to performance degradation. To address this issue, we propose a novel Interactive Text-3D Scene Retrieval Method (IDeal), which promotes the enhancement of the alignment between texts and 3D scenes through continuous interaction. To achieve this, we present an Interactive Retrieval Refinement framework (IRR), which employs a questioner to pose contextually relevant questions to an answerer in successive rounds that either promote detailed probing or encourage exploratory divergence within scenes. Upon the iterative responses received from the answerer, IRR adopts a retriever to perform both feature-level and semantic-level information fusion, facilitating scene-level interaction and understanding for more precise re-rankings. To bridge the domain gap between queries and interactive texts, we propose an Interaction Adaptation Tuning strategy (IAT). IAT mitigates the discriminability and diversity risks among augmented text features that approximate the interaction text domain, achieving contrastive domain adaptation for our retriever. Extensive experimental results on three datasets demonstrate the superiority of IDeal.</p>
            <p id="subjects-fohuurA03P@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-fohuurA03P@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fohuurA03P@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fohuurA03P@OpenReview" onclick="foldPdfKimi('fohuurA03P@OpenReview', this)" class="hr hr-fold">
        </div><div id="XoN10bZtR9@OpenReview" class="panel paper" keywords="jmmd,hsic,domain,joint,discrepancy,rethinking,graph,discrimination,maximum,mean">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XoN10bZtR9" target="_blank" title="37/77"><span class="index notranslate">#37</span></a>
                <a id="title-XoN10bZtR9@OpenReview" class="title-link" href="/venue/XoN10bZtR9@OpenReview" target="_blank">Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation</a>
                <a id="pdf-XoN10bZtR9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XoN10bZtR9@OpenReview', this)" data="https://openreview.net/pdf?id=XoN10bZtR9">[PDF<sup id="pdf-stars-XoN10bZtR9@OpenReview">9</sup>]</a>
                <a id="copy-XoN10bZtR9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XoN10bZtR9@OpenReview')">[Copy]</a>
                <a id="kimi-XoN10bZtR9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XoN10bZtR9@OpenReview', this)">[Kimi<sup id="kimi-stars-XoN10bZtR9@OpenReview">5</sup>]</a>
                <a id="rel-XoN10bZtR9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XoN10bZtR9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XoN10bZtR9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wang" target="_blank">Wei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haifeng Xia" target="_blank">Haifeng Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Huang" target="_blank">Chao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengming Ding" target="_blank">Zhengming Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cong Wang" target="_blank">Cong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haojie Li" target="_blank">Haojie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochun Cao" target="_blank">Xiaochun Cao</a>
            </p>
            <p id="summary-XoN10bZtR9@OpenReview" class="summary">In domain adaption (DA), joint maximum mean discrepancy (JMMD), as a famous distribution-distance metric, aims to measure joint probability distribution difference between the source domain and target domain, while it is still not fully explored and especially hard to be applied into a subspace-learning framework as its empirical estimation involves a tensor-product operator whose partial derivative is difficult to obtain. To solve this issue, we deduce a concise JMMD based on the Representer theorem that avoids the tensor-product operator and obtains two essential findings. First, we reveal the uniformity of JMMD by proving that previous marginal, class conditional, and weighted class conditional probability distribution distances are three special cases of JMMD with different label reproducing kernels. Second, inspired by graph embedding, we observe that the similarity weights, which strengthen the intra-class compactness in the graph of Hilbert Schmidt independence criterion (HSIC), take opposite signs in the graph of JMMD, revealing why JMMD degrades the feature discrimination. This motivates us to propose a novel loss JMMD-HSIC by jointly considering JMMD and HSIC to promote discrimination of JMMD. Extensive experiments on several cross-domain datasets could demonstrate the validity of our revealed theoretical results and the effectiveness of our proposed JMMD-HSIC.</p>
            <p id="subjects-XoN10bZtR9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-XoN10bZtR9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XoN10bZtR9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XoN10bZtR9@OpenReview" onclick="foldPdfKimi('XoN10bZtR9@OpenReview', this)" class="hr hr-fold">
        </div><div id="OzdAnGHEPx@OpenReview" class="panel paper" keywords="pan,lut,look,sharpening,table,images,15k,remote,learnable,methods">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OzdAnGHEPx" target="_blank" title="38/77"><span class="index notranslate">#38</span></a>
                <a id="title-OzdAnGHEPx@OpenReview" class="title-link" href="/venue/OzdAnGHEPx@OpenReview" target="_blank">Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables</a>
                <a id="pdf-OzdAnGHEPx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OzdAnGHEPx@OpenReview', this)" data="https://openreview.net/pdf?id=OzdAnGHEPx">[PDF<sup id="pdf-stars-OzdAnGHEPx@OpenReview">10</sup>]</a>
                <a id="copy-OzdAnGHEPx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OzdAnGHEPx@OpenReview')">[Copy]</a>
                <a id="kimi-OzdAnGHEPx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OzdAnGHEPx@OpenReview', this)">[Kimi<sup id="kimi-stars-OzdAnGHEPx@OpenReview">3</sup>]</a>
                <a id="rel-OzdAnGHEPx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OzdAnGHEPx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OzdAnGHEPx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongnan Cai" target="_blank">Zhongnan Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingying Wang" target="_blank">Yingying Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Zheng" target="_blank">Hui Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Panwang Pan" target="_blank">Panwang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=ZiXu Lin" target="_blank">ZiXu Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ge Meng" target="_blank">Ge Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxin Li" target="_blank">Chenxin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunming He" target="_blank">Chunming He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxin Xie" target="_blank">Jiaxin Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunlong Lin" target="_blank">Yunlong Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junbin Lu" target="_blank">Junbin Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Huang" target="_blank">Yue Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinghao Ding" target="_blank">Xinghao Ding</a>
            </p>
            <p id="summary-OzdAnGHEPx@OpenReview" class="summary">Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-57-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-430" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-431"><span class="mo" id="MathJax-Span-432" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-57">\times</script>15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-58-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-433" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-434"><span class="mo" id="MathJax-Span-435" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-58">\times</script>9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency.</p>
            <p id="subjects-OzdAnGHEPx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-OzdAnGHEPx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OzdAnGHEPx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OzdAnGHEPx@OpenReview" onclick="foldPdfKimi('OzdAnGHEPx@OpenReview', this)" class="hr hr-fold">
        </div><div id="ImpizBSKcu@OpenReview" class="panel paper" keywords="overfitting,decoupling,dynamical,large,inductive,complexity,generalization,layer,bias,overparametrized">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ImpizBSKcu" target="_blank" title="39/77"><span class="index notranslate">#39</span></a>
                <a id="title-ImpizBSKcu@OpenReview" class="title-link" href="/venue/ImpizBSKcu@OpenReview" target="_blank">Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks</a>
                <a id="pdf-ImpizBSKcu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ImpizBSKcu@OpenReview', this)" data="https://openreview.net/pdf?id=ImpizBSKcu">[PDF<sup id="pdf-stars-ImpizBSKcu@OpenReview">9</sup>]</a>
                <a id="copy-ImpizBSKcu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ImpizBSKcu@OpenReview')">[Copy]</a>
                <a id="kimi-ImpizBSKcu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ImpizBSKcu@OpenReview', this)">[Kimi<sup id="kimi-stars-ImpizBSKcu@OpenReview">4</sup>]</a>
                <a id="rel-ImpizBSKcu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ImpizBSKcu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ImpizBSKcu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Montanari" target="_blank">Andrea Montanari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierfrancesco Urbani" target="_blank">Pierfrancesco Urbani</a>
            </p>
            <p id="summary-ImpizBSKcu@OpenReview" class="summary">Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm. We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-59-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-436" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-437"><span class="mi" id="MathJax-Span-438" style="font-family: MathJax_Math-italic;">m</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></span></span><script type="math/tex" id="MathJax-Element-59">m</script>, and large number of samples per input dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-60-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-439" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.62em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-440"><span class="mi" id="MathJax-Span-441" style="font-family: MathJax_Math-italic;">n</span><span class="texatom" id="MathJax-Span-442"><span class="mrow" id="MathJax-Span-443"><span class="mo" id="MathJax-Span-444" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-445" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-60">n/d</script>, the training dynamics exhibits a separation of timescales which implies: <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-61-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-446" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.04em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-447"><span class="mo" id="MathJax-Span-448" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-449" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-450" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-61">(i)</script> The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network; <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-62-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-451" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.36em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-452"><span class="mo" id="MathJax-Span-453" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-454" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-455" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-456" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-62">(ii)</script> Inductive bias towards small complexity if the initialization has small enough complexity; <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-63-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-457" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.72em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-458"><span class="mo" id="MathJax-Span-459" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-460" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-461" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-462" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-463" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>i</mi><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-63">(iii)</script> A dynamical decoupling between feature learning and overfitting regimes; <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-64-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-464" style="width: 1.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.51em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-465"><span class="mo" id="MathJax-Span-466" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-467" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-468" style="font-family: MathJax_Math-italic;">v</span><span class="mo" id="MathJax-Span-469" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>v</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-64">(iv)</script> A non-monotone behavior of the test error, associated `feature unlearning' regime at large times.</p>
            <p id="subjects-ImpizBSKcu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-ImpizBSKcu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ImpizBSKcu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ImpizBSKcu@OpenReview" onclick="foldPdfKimi('ImpizBSKcu@OpenReview', this)" class="hr hr-fold">
        </div><div id="s0JVsx3bx1@OpenReview" class="panel paper" keywords="depth,self,supervised,goal,reaching,conditioned,commanded,scaling,layers,increases">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=s0JVsx3bx1" target="_blank" title="40/77"><span class="index notranslate">#40</span></a>
                <a id="title-s0JVsx3bx1@OpenReview" class="title-link" href="/venue/s0JVsx3bx1@OpenReview" target="_blank">1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</a>
                <a id="pdf-s0JVsx3bx1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('s0JVsx3bx1@OpenReview', this)" data="https://openreview.net/pdf?id=s0JVsx3bx1">[PDF<sup id="pdf-stars-s0JVsx3bx1@OpenReview">11</sup>]</a>
                <a id="copy-s0JVsx3bx1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('s0JVsx3bx1@OpenReview')">[Copy]</a>
                <a id="kimi-s0JVsx3bx1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('s0JVsx3bx1@OpenReview', this)">[Kimi<sup id="kimi-stars-s0JVsx3bx1@OpenReview">4</sup>]</a>
                <a id="rel-s0JVsx3bx1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('s0JVsx3bx1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-s0JVsx3bx1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Wang" target="_blank">Kevin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ishaan Javali" target="_blank">Ishaan Javali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michał Bortkiewicz" target="_blank">Michał Bortkiewicz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomasz Trzcinski" target="_blank">Tomasz Trzcinski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Eysenbach" target="_blank">Benjamin Eysenbach</a>
            </p>
            <p id="summary-s0JVsx3bx1@OpenReview" class="summary">Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-65-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-470" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-471"><span class="mn" id="MathJax-Span-472" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-473" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-65">2\times</script> -- <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-66-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;50&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-474" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-475"><span class="mn" id="MathJax-Span-476" style="font-family: MathJax_Main;">50</span><span class="mo" id="MathJax-Span-477" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>50</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-66">50\times</script>, outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.</p>
            <p id="subjects-s0JVsx3bx1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-s0JVsx3bx1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-s0JVsx3bx1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-s0JVsx3bx1@OpenReview" onclick="foldPdfKimi('s0JVsx3bx1@OpenReview', this)" class="hr hr-fold">
        </div><div id="XO9fhSZkBh@OpenReview" class="panel paper" keywords="maxout,networks,braid,hidden,layers,neural,bound,maximum,numbers,arrangement">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XO9fhSZkBh" target="_blank" title="41/77"><span class="index notranslate">#41</span></a>
                <a id="title-XO9fhSZkBh@OpenReview" class="title-link" href="/venue/XO9fhSZkBh@OpenReview" target="_blank">Depth-Bounds for Neural Networks via the Braid Arrangement</a>
                <a id="pdf-XO9fhSZkBh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XO9fhSZkBh@OpenReview', this)" data="https://openreview.net/pdf?id=XO9fhSZkBh">[PDF<sup id="pdf-stars-XO9fhSZkBh@OpenReview">4</sup>]</a>
                <a id="copy-XO9fhSZkBh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XO9fhSZkBh@OpenReview')">[Copy]</a>
                <a id="kimi-XO9fhSZkBh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XO9fhSZkBh@OpenReview', this)">[Kimi<sup id="kimi-stars-XO9fhSZkBh@OpenReview">3</sup>]</a>
                <a id="rel-XO9fhSZkBh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XO9fhSZkBh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XO9fhSZkBh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Leo Grillo" target="_blank">Moritz Leo Grillo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Hertrich" target="_blank">Christoph Hertrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georg Loho" target="_blank">Georg Loho</a>
            </p>
            <p id="summary-XO9fhSZkBh@OpenReview" class="summary">We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-67-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-478" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-479"><span class="msubsup" id="MathJax-Span-480"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.607em, -999.997em); top: -2.445em; left: 0em;"><span class="texatom" id="MathJax-Span-481"><span class="mrow" id="MathJax-Span-482"><span class="mi" id="MathJax-Span-483" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.732em;"><span class="mi" id="MathJax-Span-484" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-67">\mathbb{R}^d</script>. While the question has been resolved in special cases, the best known lower bound in general is still 2. We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan. For such neural networks, we prove a non-constant lower bound of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-68-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-485" style="width: 5.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.79em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-486"><span class="mi" id="MathJax-Span-487" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-488" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-489" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-490"></span><span class="mi" id="MathJax-Span-491" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-492"></span><span class="mi" id="MathJax-Span-493" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-494" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-68">\Omega(\log\log d)</script> hidden layers required to exactly represent the maximum of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-69-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-495" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-496"><span class="mi" id="MathJax-Span-497" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-69">d</script> numbers. Additionally, we provide a combinatorial proof that neural networks satisfying this assumption require three hidden layers to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far. Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers.</p>
            <p id="subjects-XO9fhSZkBh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-XO9fhSZkBh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XO9fhSZkBh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XO9fhSZkBh@OpenReview" onclick="foldPdfKimi('XO9fhSZkBh@OpenReview', this)" class="hr hr-fold">
        </div><div id="VYLdKb5dzO@OpenReview" class="panel paper" keywords="cmi,generalization,bounds,memorize,tighter,memorization,attias,livni,projection,algorithm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VYLdKb5dzO" target="_blank" title="42/77"><span class="index notranslate">#42</span></a>
                <a id="title-VYLdKb5dzO@OpenReview" class="title-link" href="/venue/VYLdKb5dzO@OpenReview" target="_blank">Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization</a>
                <a id="pdf-VYLdKb5dzO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VYLdKb5dzO@OpenReview', this)" data="https://openreview.net/pdf?id=VYLdKb5dzO">[PDF<sup id="pdf-stars-VYLdKb5dzO@OpenReview">4</sup>]</a>
                <a id="copy-VYLdKb5dzO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VYLdKb5dzO@OpenReview')">[Copy]</a>
                <a id="kimi-VYLdKb5dzO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VYLdKb5dzO@OpenReview', this)">[Kimi<sup id="kimi-stars-VYLdKb5dzO@OpenReview">5</sup>]</a>
                <a id="rel-VYLdKb5dzO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VYLdKb5dzO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VYLdKb5dzO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Milad Sefidgaran" target="_blank">Milad Sefidgaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kimia Nadjahi" target="_blank">Kimia Nadjahi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdellatif Zaidi" target="_blank">Abdellatif Zaidi</a>
            </p>
            <p id="summary-VYLdKb5dzO@OpenReview" class="summary">In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-70-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-498" style="width: 4.898em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.065em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.96em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-499"><span class="texatom" id="MathJax-Span-500"><span class="mrow" id="MathJax-Span-501"><span class="mi" id="MathJax-Span-502" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-503" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-504" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-505"><span class="mrow" id="MathJax-Span-506"><span class="mo" id="MathJax-Span-507" style="font-family: MathJax_Main;">/</span></span></span><span class="msqrt" id="MathJax-Span-508"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-509"><span class="mi" id="MathJax-Span-510" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-511" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-70">\mathcal{O}(1/\sqrt{n})</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-71-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-512" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-513"><span class="mi" id="MathJax-Span-514" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-71">n</script> is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data "memorization" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must "memorize'' a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.</p>
            <p id="subjects-VYLdKb5dzO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-VYLdKb5dzO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VYLdKb5dzO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VYLdKb5dzO@OpenReview" onclick="foldPdfKimi('VYLdKb5dzO@OpenReview', this)" class="hr hr-fold">
        </div><div id="sYK4yPDuT1@OpenReview" class="panel paper" keywords="online,attribution,iif,training,policy,framework,reinforcement,snapshot,updates,filtering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sYK4yPDuT1" target="_blank" title="43/77"><span class="index notranslate">#43</span></a>
                <a id="title-sYK4yPDuT1@OpenReview" class="title-link" href="/venue/sYK4yPDuT1@OpenReview" target="_blank">A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning</a>
                <a id="pdf-sYK4yPDuT1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sYK4yPDuT1@OpenReview', this)" data="https://openreview.net/pdf?id=sYK4yPDuT1">[PDF<sup id="pdf-stars-sYK4yPDuT1@OpenReview">9</sup>]</a>
                <a id="copy-sYK4yPDuT1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sYK4yPDuT1@OpenReview')">[Copy]</a>
                <a id="kimi-sYK4yPDuT1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sYK4yPDuT1@OpenReview', this)">[Kimi<sup id="kimi-stars-sYK4yPDuT1@OpenReview">4</sup>]</a>
                <a id="rel-sYK4yPDuT1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sYK4yPDuT1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sYK4yPDuT1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzheng Hu" target="_blank">Yuzheng Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Wu" target="_blank">Fan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Ye" target="_blank">Haotian Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Forsyth" target="_blank">David Forsyth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Zou" target="_blank">James Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Jiang" target="_blank">Nan Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi W. Ma" target="_blank">Jiaqi W. Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Zhao" target="_blank">Han Zhao</a>
            </p>
            <p id="summary-sYK4yPDuT1@OpenReview" class="summary">Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective.</p>
            <p id="subjects-sYK4yPDuT1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-sYK4yPDuT1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sYK4yPDuT1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sYK4yPDuT1@OpenReview" onclick="foldPdfKimi('sYK4yPDuT1@OpenReview', this)" class="hr hr-fold">
        </div><div id="cGks3s79hW@OpenReview" class="panel paper" keywords="dimensional,neuronal,activity,latents,low,latent,neurons,high,cortex,solvable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cGks3s79hW" target="_blank" title="44/77"><span class="index notranslate">#44</span></a>
                <a id="title-cGks3s79hW@OpenReview" class="title-link" href="/venue/cGks3s79hW@OpenReview" target="_blank">High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model</a>
                <a id="pdf-cGks3s79hW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cGks3s79hW@OpenReview', this)" data="https://openreview.net/pdf?id=cGks3s79hW">[PDF<sup id="pdf-stars-cGks3s79hW@OpenReview">8</sup>]</a>
                <a id="copy-cGks3s79hW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cGks3s79hW@OpenReview')">[Copy]</a>
                <a id="kimi-cGks3s79hW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cGks3s79hW@OpenReview', this)">[Kimi<sup id="kimi-stars-cGks3s79hW@OpenReview">3</sup>]</a>
                <a id="rel-cGks3s79hW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cGks3s79hW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cGks3s79hW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Valentin Schmutz" target="_blank">Valentin Schmutz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Haydaroğlu" target="_blank">Ali Haydaroğlu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuqi Wang" target="_blank">Shuqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixiao Feng" target="_blank">Yixiao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Carandini" target="_blank">Matteo Carandini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kenneth D. Harris" target="_blank">Kenneth D. Harris</a>
            </p>
            <p id="summary-cGks3s79hW@OpenReview" class="summary">Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.</p>
            <p id="subjects-cGks3s79hW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-cGks3s79hW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cGks3s79hW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cGks3s79hW@OpenReview" onclick="foldPdfKimi('cGks3s79hW@OpenReview', this)" class="hr hr-fold">
        </div><div id="oGmROC4e4W@OpenReview" class="panel paper" keywords="spiking,slopes,surrogate,snns,gradients,shallower,scheduled,training,td3bc,neuromorphic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oGmROC4e4W" target="_blank" title="45/77"><span class="index notranslate">#45</span></a>
                <a id="title-oGmROC4e4W@OpenReview" class="title-link" href="/venue/oGmROC4e4W@OpenReview" target="_blank">Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks</a>
                <a id="pdf-oGmROC4e4W@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oGmROC4e4W@OpenReview', this)" data="https://openreview.net/pdf?id=oGmROC4e4W">[PDF<sup id="pdf-stars-oGmROC4e4W@OpenReview">7</sup>]</a>
                <a id="copy-oGmROC4e4W@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oGmROC4e4W@OpenReview')">[Copy]</a>
                <a id="kimi-oGmROC4e4W@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oGmROC4e4W@OpenReview', this)">[Kimi<sup id="kimi-stars-oGmROC4e4W@OpenReview">4</sup>]</a>
                <a id="rel-oGmROC4e4W@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oGmROC4e4W@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oGmROC4e4W@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Korneel Van den Berghe" target="_blank">Korneel Van den Berghe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stein Stroobants" target="_blank">Stein Stroobants</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vijay Janapa Reddi" target="_blank">Vijay Janapa Reddi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guido De Croon" target="_blank">Guido De Croon</a>
            </p>
            <p id="summary-oGmROC4e4W@OpenReview" class="summary">Neuromorphic computing systems are set to revolutionize energy-constrained robotics by achieving orders-of-magnitude efficiency gains, while enabling native temporal processing. Spiking Neural Networks (SNNs) represent a promising algorithmic approach for these systems, yet their application to complex control tasks faces two critical challenges: (1) the non-differentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties, and (2) the stateful dynamics of SNNs require training on sequences, which in reinforcement learning (RL) is hindered by limited sequence lengths during early training, preventing the network from bridging its warm-up period. We address these challenges by systematically analyzing surrogate gradient slope settings, showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients. In supervised learning, we find no clear preference for fixed or scheduled slopes. The effect is much more pronounced in RL settings, where shallower slopes or scheduled slopes lead to a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-72-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;2.1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-515" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-516"><span class="mo" id="MathJax-Span-517" style="font-family: MathJax_Main;">×</span><span class="mn" id="MathJax-Span-518" style="font-family: MathJax_Main;">2.1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo><mn>2.1</mn></math></span></span><script type="math/tex" id="MathJax-Element-72">\times2.1</script> improvement in both training and final deployed performance. Next, we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process, while still exploiting online environment interactions with the spiking policy. Combining our method with an adaptive slope schedule for a real-world drone position control task, we achieve an average return of 400 points, substantially outperforming prior techniques, including Behavioral Cloning and TD3BC, which achieve at most –200 points under the same conditions. This work advances both the theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers demonstrated in real-world robotic systems.</p>
            <p id="subjects-oGmROC4e4W@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-oGmROC4e4W@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oGmROC4e4W@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oGmROC4e4W@OpenReview" onclick="foldPdfKimi('oGmROC4e4W@OpenReview', this)" class="hr hr-fold">
        </div><div id="aUAG1WS7J2@OpenReview" class="panel paper" keywords="replay,class,fedcbdr,fcil,imbalance,wise,tasks,global,task,balancing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aUAG1WS7J2" target="_blank" title="46/77"><span class="index notranslate">#46</span></a>
                <a id="title-aUAG1WS7J2@OpenReview" class="title-link" href="/venue/aUAG1WS7J2@OpenReview" target="_blank">Class-wise Balancing Data Replay for Federated Class-Incremental Learning</a>
                <a id="pdf-aUAG1WS7J2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aUAG1WS7J2@OpenReview', this)" data="https://openreview.net/pdf?id=aUAG1WS7J2">[PDF<sup id="pdf-stars-aUAG1WS7J2@OpenReview">8</sup>]</a>
                <a id="copy-aUAG1WS7J2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aUAG1WS7J2@OpenReview')">[Copy]</a>
                <a id="kimi-aUAG1WS7J2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aUAG1WS7J2@OpenReview', this)">[Kimi<sup id="kimi-stars-aUAG1WS7J2@OpenReview">4</sup>]</a>
                <a id="rel-aUAG1WS7J2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aUAG1WS7J2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aUAG1WS7J2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuang Qi" target="_blank">Zhuang Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying-Peng Tang" target="_blank">Ying-Peng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Meng" target="_blank">Lei Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Yu" target="_blank">Han Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxiao Li" target="_blank">Xiaoxiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangxu Meng" target="_blank">Xiangxu Meng</a>
            </p>
            <p id="summary-aUAG1WS7J2@OpenReview" class="summary">Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model’s overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.</p>
            <p id="subjects-aUAG1WS7J2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-aUAG1WS7J2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aUAG1WS7J2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aUAG1WS7J2@OpenReview" onclick="foldPdfKimi('aUAG1WS7J2@OpenReview', this)" class="hr hr-fold">
        </div><div id="m7MD0sa8Re@OpenReview" class="panel paper" keywords="tactile,ead,rodent,convrnn,recurrent,somatosensory,neural,categorization,cortex,supervised">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=m7MD0sa8Re" target="_blank" title="47/77"><span class="index notranslate">#47</span></a>
                <a id="title-m7MD0sa8Re@OpenReview" class="title-link" href="/venue/m7MD0sa8Re@OpenReview" target="_blank">Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</a>
                <a id="pdf-m7MD0sa8Re@OpenReview" class="title-pdf notranslate" onclick="togglePdf('m7MD0sa8Re@OpenReview', this)" data="https://openreview.net/pdf?id=m7MD0sa8Re">[PDF<sup id="pdf-stars-m7MD0sa8Re@OpenReview">6</sup>]</a>
                <a id="copy-m7MD0sa8Re@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('m7MD0sa8Re@OpenReview')">[Copy]</a>
                <a id="kimi-m7MD0sa8Re@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('m7MD0sa8Re@OpenReview', this)">[Kimi<sup id="kimi-stars-m7MD0sa8Re@OpenReview">4</sup>]</a>
                <a id="rel-m7MD0sa8Re@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('m7MD0sa8Re@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-m7MD0sa8Re@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Trinity Chung" target="_blank">Trinity Chung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Shen" target="_blank">Yuchen Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Kong" target="_blank">Nathan Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aran Nayebi" target="_blank">Aran Nayebi</a>
            </p>
            <p id="summary-m7MD0sa8Re@OpenReview" class="summary">Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.</p>
            <p id="subjects-m7MD0sa8Re@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-m7MD0sa8Re@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-m7MD0sa8Re@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-m7MD0sa8Re@OpenReview" onclick="foldPdfKimi('m7MD0sa8Re@OpenReview', this)" class="hr hr-fold">
        </div><div id="Zd6VyjmN1S@OpenReview" class="panel paper" keywords="serving,multimodal,elasticmm,parallelism,inference,ttft,emp,mllms,stages,elastic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Zd6VyjmN1S" target="_blank" title="48/77"><span class="index notranslate">#48</span></a>
                <a id="title-Zd6VyjmN1S@OpenReview" class="title-link" href="/venue/Zd6VyjmN1S@OpenReview" target="_blank">ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism</a>
                <a id="pdf-Zd6VyjmN1S@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Zd6VyjmN1S@OpenReview', this)" data="https://openreview.net/pdf?id=Zd6VyjmN1S">[PDF<sup id="pdf-stars-Zd6VyjmN1S@OpenReview">12</sup>]</a>
                <a id="copy-Zd6VyjmN1S@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Zd6VyjmN1S@OpenReview')">[Copy]</a>
                <a id="kimi-Zd6VyjmN1S@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Zd6VyjmN1S@OpenReview', this)">[Kimi<sup id="kimi-stars-Zd6VyjmN1S@OpenReview">7</sup>]</a>
                <a id="rel-Zd6VyjmN1S@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Zd6VyjmN1S@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Zd6VyjmN1S@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zedong Liu" target="_blank">Zedong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shenggan Cheng" target="_blank">Shenggan Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangming Tan" target="_blank">Guangming Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang You" target="_blank">Yang You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dingwen Tao" target="_blank">Dingwen Tao</a>
            </p>
            <p id="summary-Zd6VyjmN1S@OpenReview" class="summary">Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-73-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-519" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-520"><span class="mo" id="MathJax-Span-521" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-73">\times</script> and achieving 3.2–4.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-74-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-522" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-523"><span class="mo" id="MathJax-Span-524" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-74">\times</script> higher throughput while meeting service-level objectives (SLOs).</p>
            <p id="subjects-Zd6VyjmN1S@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zd6VyjmN1S@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zd6VyjmN1S@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zd6VyjmN1S@OpenReview" onclick="foldPdfKimi('Zd6VyjmN1S@OpenReview', this)" class="hr hr-fold">
        </div><div id="7AwFJzgIUW@OpenReview" class="panel paper" keywords="adversarial,rank,compression,robustness,attacks,low,networks,dynamical,uncompressed,regularizer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7AwFJzgIUW" target="_blank" title="49/77"><span class="index notranslate">#49</span></a>
                <a id="title-7AwFJzgIUW@OpenReview" class="title-link" href="/venue/7AwFJzgIUW@OpenReview" target="_blank">Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks</a>
                <a id="pdf-7AwFJzgIUW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7AwFJzgIUW@OpenReview', this)" data="https://openreview.net/pdf?id=7AwFJzgIUW">[PDF<sup id="pdf-stars-7AwFJzgIUW@OpenReview">8</sup>]</a>
                <a id="copy-7AwFJzgIUW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7AwFJzgIUW@OpenReview')">[Copy]</a>
                <a id="kimi-7AwFJzgIUW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7AwFJzgIUW@OpenReview', this)">[Kimi<sup id="kimi-stars-7AwFJzgIUW@OpenReview">6</sup>]</a>
                <a id="rel-7AwFJzgIUW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7AwFJzgIUW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7AwFJzgIUW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Steffen Schotthöfer" target="_blank">Steffen Schotthöfer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=H. Lexie Yang" target="_blank">H. Lexie Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Schnake" target="_blank">Stefan Schnake</a>
            </p>
            <p id="summary-7AwFJzgIUW@OpenReview" class="summary">Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines.</p>
            <p id="subjects-7AwFJzgIUW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-7AwFJzgIUW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7AwFJzgIUW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7AwFJzgIUW@OpenReview" onclick="foldPdfKimi('7AwFJzgIUW@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZwCVFBFUFb@OpenReview" class="panel paper" keywords="qoq,med,clinical,grpo,drpo,multimodal,foundation,domain,training,aware">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZwCVFBFUFb" target="_blank" title="50/77"><span class="index notranslate">#50</span></a>
                <a id="title-ZwCVFBFUFb@OpenReview" class="title-link" href="/venue/ZwCVFBFUFb@OpenReview" target="_blank">QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</a>
                <a id="pdf-ZwCVFBFUFb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZwCVFBFUFb@OpenReview', this)" data="https://openreview.net/pdf?id=ZwCVFBFUFb">[PDF<sup id="pdf-stars-ZwCVFBFUFb@OpenReview">12</sup>]</a>
                <a id="copy-ZwCVFBFUFb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZwCVFBFUFb@OpenReview')">[Copy]</a>
                <a id="kimi-ZwCVFBFUFb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZwCVFBFUFb@OpenReview', this)">[Kimi<sup id="kimi-stars-ZwCVFBFUFb@OpenReview">10</sup>]</a>
                <a id="rel-ZwCVFBFUFb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZwCVFBFUFb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZwCVFBFUFb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Dai" target="_blank">Wei Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peilin Chen" target="_blank">Peilin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chanakya Ekbote" target="_blank">Chanakya Ekbote</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Pu Liang" target="_blank">Paul Pu Liang</a>
            </p>
            <p id="summary-ZwCVFBFUFb@OpenReview" class="summary">Clinical decision‑making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision‑centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time‑series signals, and text reports. QoQ-Med is trained with Domain‑aware Relative Policy Optimization (DRPO), a novel reinforcement‑learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro‑F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces.</p>
            <p id="subjects-ZwCVFBFUFb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-ZwCVFBFUFb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZwCVFBFUFb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZwCVFBFUFb@OpenReview" onclick="foldPdfKimi('ZwCVFBFUFb@OpenReview', this)" class="hr hr-fold">
        </div><div id="1b7whO4SfY@OpenReview" class="panel paper" keywords="gating,attention,sdpa,gated,softmax,sink,qwen3,variants,huggingface,linearity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1b7whO4SfY" target="_blank" title="51/77"><span class="index notranslate">#51</span></a>
                <a id="title-1b7whO4SfY@OpenReview" class="title-link" href="/venue/1b7whO4SfY@OpenReview" target="_blank">Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free</a>
                <a id="pdf-1b7whO4SfY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1b7whO4SfY@OpenReview', this)" data="https://openreview.net/pdf?id=1b7whO4SfY">[PDF<sup id="pdf-stars-1b7whO4SfY@OpenReview">39</sup>]</a>
                <a id="copy-1b7whO4SfY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1b7whO4SfY@OpenReview')">[Copy]</a>
                <a id="kimi-1b7whO4SfY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1b7whO4SfY@OpenReview', this)">[Kimi<sup id="kimi-stars-1b7whO4SfY@OpenReview">22</sup>]</a>
                <a id="rel-1b7whO4SfY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1b7whO4SfY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1b7whO4SfY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Qiu" target="_blank">Zihan Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zekun Wang" target="_blank">Zekun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zheng" target="_blank">Bo Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Huang" target="_blank">Zeyu Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiyue Wen" target="_blank">Kaiyue Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songlin Yang" target="_blank">Songlin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Men" target="_blank">Rui Men</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Yu" target="_blank">Le Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Huang" target="_blank">Fei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suozhi Huang" target="_blank">Suozhi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dayiheng Liu" target="_blank">Dayiheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingren Zhou" target="_blank">Jingren Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyang Lin" target="_blank">Junyang Lin</a>
            </p>
            <p id="summary-1b7whO4SfY@OpenReview" class="summary">Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).</p>
            <p id="subjects-1b7whO4SfY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-1b7whO4SfY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1b7whO4SfY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1b7whO4SfY@OpenReview" onclick="foldPdfKimi('1b7whO4SfY@OpenReview', this)" class="hr hr-fold">
        </div><div id="w1ihNiIBOc@OpenReview" class="panel paper" keywords="rhel,emph,hrus,hamiltonian,bptt,hssms,ssms,systems,physical,recurrent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=w1ihNiIBOc" target="_blank" title="52/77"><span class="index notranslate">#52</span></a>
                <a id="title-w1ihNiIBOc@OpenReview" class="title-link" href="/venue/w1ihNiIBOc@OpenReview" target="_blank">Learning long range dependencies through time reversal symmetry breaking</a>
                <a id="pdf-w1ihNiIBOc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('w1ihNiIBOc@OpenReview', this)" data="https://openreview.net/pdf?id=w1ihNiIBOc">[PDF<sup id="pdf-stars-w1ihNiIBOc@OpenReview">7</sup>]</a>
                <a id="copy-w1ihNiIBOc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('w1ihNiIBOc@OpenReview')">[Copy]</a>
                <a id="kimi-w1ihNiIBOc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('w1ihNiIBOc@OpenReview', this)">[Kimi<sup id="kimi-stars-w1ihNiIBOc@OpenReview">3</sup>]</a>
                <a id="rel-w1ihNiIBOc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('w1ihNiIBOc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-w1ihNiIBOc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guillaume Pourcel" target="_blank">Guillaume Pourcel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maxence Ernoult" target="_blank">Maxence Ernoult</a>
            </p>
            <p id="summary-w1ihNiIBOc@OpenReview" class="summary">Deep State Space Models (SSMs) reignite physics-grounded compute paradigms, as RNNs could natively be embodied into dynamical systems. This calls for dedicated learning algorithms obeying to core physical principles, with efficient techniques to simulate these systems and guide their design. We propose \emph{Recurrent Hamiltonian Echo Learning} (RHEL), an algorithm which provably computes loss gradients as finite differences of physical trajectories of non-dissipative, \emph{Hamiltonian systems}. In ML terms, RHEL only requires three ``forward passes'' irrespective of model size, without explicit Jacobian computation, nor incurring any variance in the gradient estimation. Motivated by the potential to implement our algorithm in non-digital physical systems, we first introduce RHEL in \emph{continuous time} and demonstrate its formal equivalence with the continuous adjoint state method. To facilitate the simulation of Hamiltonian systems trained by RHEL, we propose a \emph{discrete-time} version of RHEL which is equivalent to Backpropagation Through Time (BPTT) when applied to a class of recurrent modules which we call \emph{Hamiltonian Recurrent Units} (HRUs). This setting allows us to demonstrate the scalability of RHEL by generalizing these results to hierarchies of HRUs, which we call \emph{Hamiltonian SSMs} (HSSMs). We apply RHEL to train HSSMs with linear and nonlinear dynamics on a variety of time-series tasks ranging from mid-range to long-range classification and regression with sequence length reaching <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-75-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mn&gt;50&lt;/mn&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-525" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.55em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-526"><span class="mo" id="MathJax-Span-527" style="font-family: MathJax_Main;">∼</span><span class="mn" id="MathJax-Span-528" style="font-family: MathJax_Main; padding-left: 0.263em;">50</span><span class="mi" id="MathJax-Span-529" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mn>50</mn><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-75">\sim 50k</script>. We show that RHEL consistently matches the performance of BPTT across all models and tasks. This work opens new doors for the design of scalable, energy-efficient physical systems endowed with self-learning capabilities for sequence modelling.</p>
            <p id="subjects-w1ihNiIBOc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-w1ihNiIBOc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-w1ihNiIBOc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-w1ihNiIBOc@OpenReview" onclick="foldPdfKimi('w1ihNiIBOc@OpenReview', this)" class="hr hr-fold">
        </div><div id="WJujF9An5L@OpenReview" class="panel paper" keywords="ocean,fuxi,forecasting,daily,resolution,sub,eddy,depths,resolving,predictions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WJujF9An5L" target="_blank" title="53/77"><span class="index notranslate">#53</span></a>
                <a id="title-WJujF9An5L@OpenReview" class="title-link" href="/venue/WJujF9An5L@OpenReview" target="_blank">FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution</a>
                <a id="pdf-WJujF9An5L@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WJujF9An5L@OpenReview', this)" data="https://openreview.net/pdf?id=WJujF9An5L">[PDF<sup id="pdf-stars-WJujF9An5L@OpenReview">6</sup>]</a>
                <a id="copy-WJujF9An5L@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WJujF9An5L@OpenReview')">[Copy]</a>
                <a id="kimi-WJujF9An5L@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WJujF9An5L@OpenReview', this)">[Kimi<sup id="kimi-stars-WJujF9An5L@OpenReview">1</sup>]</a>
                <a id="rel-WJujF9An5L@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WJujF9An5L@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WJujF9An5L@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiusheng Huang" target="_blank">Qiusheng Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Niu" target="_blank">Yuan Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohui Zhong" target="_blank">Xiaohui Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=AnboyuGuo" target="_blank">AnboyuGuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Chen" target="_blank">Lei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=dianjun zhang" target="_blank">dianjun zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuefeng Zhang" target="_blank">Xuefeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>
            </p>
            <p id="summary-WJujF9An5L@OpenReview" class="summary">Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12° spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths.</p>
            <p id="subjects-WJujF9An5L@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-WJujF9An5L@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WJujF9An5L@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WJujF9An5L@OpenReview" onclick="foldPdfKimi('WJujF9An5L@OpenReview', this)" class="hr hr-fold">
        </div><div id="knPz7gtjPW@OpenReview" class="panel paper" keywords="superposition,scaling,loss,neural,law,llms,laws,inversely,representation,chinchilla">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=knPz7gtjPW" target="_blank" title="54/77"><span class="index notranslate">#54</span></a>
                <a id="title-knPz7gtjPW@OpenReview" class="title-link" href="/venue/knPz7gtjPW@OpenReview" target="_blank">Superposition Yields Robust Neural Scaling</a>
                <a id="pdf-knPz7gtjPW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('knPz7gtjPW@OpenReview', this)" data="https://openreview.net/pdf?id=knPz7gtjPW">[PDF<sup id="pdf-stars-knPz7gtjPW@OpenReview">8</sup>]</a>
                <a id="copy-knPz7gtjPW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('knPz7gtjPW@OpenReview')">[Copy]</a>
                <a id="kimi-knPz7gtjPW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('knPz7gtjPW@OpenReview', this)">[Kimi<sup id="kimi-stars-knPz7gtjPW@OpenReview">6</sup>]</a>
                <a id="rel-knPz7gtjPW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('knPz7gtjPW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-knPz7gtjPW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Liu" target="_blank">Yizhou Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziming Liu" target="_blank">Ziming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeff Gore" target="_blank">Jeff Gore</a>
            </p>
            <p id="summary-knPz7gtjPW@OpenReview" class="summary">The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.</p>
            <p id="subjects-knPz7gtjPW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-knPz7gtjPW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-knPz7gtjPW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-knPz7gtjPW@OpenReview" onclick="foldPdfKimi('knPz7gtjPW@OpenReview', this)" class="hr hr-fold">
        </div><div id="i5WnXNjwbR@OpenReview" class="panel paper" keywords="reliance,texture,cnns,biased,suppression,feature,geirhos,controlled,inherently,shape">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=i5WnXNjwbR" target="_blank" title="55/77"><span class="index notranslate">#55</span></a>
                <a id="title-i5WnXNjwbR@OpenReview" class="title-link" href="/venue/i5WnXNjwbR@OpenReview" target="_blank">ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression</a>
                <a id="pdf-i5WnXNjwbR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('i5WnXNjwbR@OpenReview', this)" data="https://openreview.net/pdf?id=i5WnXNjwbR">[PDF<sup id="pdf-stars-i5WnXNjwbR@OpenReview">11</sup>]</a>
                <a id="copy-i5WnXNjwbR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('i5WnXNjwbR@OpenReview')">[Copy]</a>
                <a id="kimi-i5WnXNjwbR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('i5WnXNjwbR@OpenReview', this)">[Kimi<sup id="kimi-stars-i5WnXNjwbR@OpenReview">9</sup>]</a>
                <a id="rel-i5WnXNjwbR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('i5WnXNjwbR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-i5WnXNjwbR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Burgert" target="_blank">Tom Burgert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Stoll" target="_blank">Oliver Stoll</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paolo Rota" target="_blank">Paolo Rota</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Begüm Demir" target="_blank">Begüm Demir</a>
            </p>
            <p id="summary-i5WnXNjwbR@OpenReview" class="summary">The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.</p>
            <p id="subjects-i5WnXNjwbR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-i5WnXNjwbR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-i5WnXNjwbR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-i5WnXNjwbR@OpenReview" onclick="foldPdfKimi('i5WnXNjwbR@OpenReview', this)" class="hr hr-fold">
        </div><div id="RF3miSqdXa@OpenReview" class="panel paper" keywords="moe,lmc,architectures,experts,gating,connectivity,mixture,loss,neural,expert">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RF3miSqdXa" target="_blank" title="56/77"><span class="index notranslate">#56</span></a>
                <a id="title-RF3miSqdXa@OpenReview" class="title-link" href="/venue/RF3miSqdXa@OpenReview" target="_blank">On Linear Mode Connectivity of Mixture-of-Experts Architectures</a>
                <a id="pdf-RF3miSqdXa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RF3miSqdXa@OpenReview', this)" data="https://openreview.net/pdf?id=RF3miSqdXa">[PDF<sup id="pdf-stars-RF3miSqdXa@OpenReview">9</sup>]</a>
                <a id="copy-RF3miSqdXa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RF3miSqdXa@OpenReview')">[Copy]</a>
                <a id="kimi-RF3miSqdXa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RF3miSqdXa@OpenReview', this)">[Kimi<sup id="kimi-stars-RF3miSqdXa@OpenReview">6</sup>]</a>
                <a id="rel-RF3miSqdXa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RF3miSqdXa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RF3miSqdXa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Viet-Hoang Tran" target="_blank">Viet-Hoang Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Van-Hoan Trinh" target="_blank">Van-Hoan Trinh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khanh Vinh Bui" target="_blank">Khanh Vinh Bui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tan Minh Nguyen" target="_blank">Tan Minh Nguyen</a>
            </p>
            <p id="summary-RF3miSqdXa@OpenReview" class="summary">Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes of neural networks, wherein independently trained models have been observed to be connected—up to permutation symmetries—by linear paths in parameter space along which the loss remains consistently low. This observation challenges classical views of non-convex optimization and has implications for model ensembling, generalization, and our understanding of neural loss geometry. Inspired by recent studies on LMC in standard neural networks, we systematically investigate this phenomenon within Mixture-of-Experts (MoE) architectures—a class of models known for their scalability and computational efficiency, which combine traditional neural networks—referred to as experts—through a learnable gating mechanism. We begin by conducting a comprehensive analysis of both dense and sparse gating regimes, demonstrating that the symmetries inherent to MoE architectures are fully characterized by permutations acting on both the expert components and the gating function. Building on these foundational findings, we propose a matching algorithm that enables alignment between independently trained MoEs, thereby facilitating the discovery of LMC. Finally, we empirically validate the presence of LMC using our proposed algorithm across diverse MoE configurations—including dense, sparse, and shared-expert variants—under a wide range of model settings and datasets of varying scales and modalities. Our results confirm the existence of LMC in MoE architectures and offer fundamental insights into the functional landscape and optimization dynamics of deep learning models.</p>
            <p id="subjects-RF3miSqdXa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-RF3miSqdXa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RF3miSqdXa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RF3miSqdXa@OpenReview" onclick="foldPdfKimi('RF3miSqdXa@OpenReview', this)" class="hr hr-fold">
        </div><div id="0biUwyjKkm@OpenReview" class="panel paper" keywords="openhoi,affordance,instructions,language,hoi,multimodal,open,object,hand,synthesis">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0biUwyjKkm" target="_blank" title="57/77"><span class="index notranslate">#57</span></a>
                <a id="title-0biUwyjKkm@OpenReview" class="title-link" href="/venue/0biUwyjKkm@OpenReview" target="_blank">OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model</a>
                <a id="pdf-0biUwyjKkm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0biUwyjKkm@OpenReview', this)" data="https://openreview.net/pdf?id=0biUwyjKkm">[PDF<sup id="pdf-stars-0biUwyjKkm@OpenReview">12</sup>]</a>
                <a id="copy-0biUwyjKkm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0biUwyjKkm@OpenReview')">[Copy]</a>
                <a id="kimi-0biUwyjKkm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0biUwyjKkm@OpenReview', this)">[Kimi<sup id="kimi-stars-0biUwyjKkm@OpenReview">4</sup>]</a>
                <a id="rel-0biUwyjKkm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0biUwyjKkm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0biUwyjKkm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenhao Zhang" target="_blank">Zhenhao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Shi" target="_blank">Ye Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingxiao Yang" target="_blank">Lingxiao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suting Ni" target="_blank">Suting Ni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Ye" target="_blank">Qi Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingya Wang" target="_blank">Jingya Wang</a>
            </p>
            <p id="summary-0biUwyjKkm@OpenReview" class="summary">Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., “Find a water bottle and take a sip”) into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI’s superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions.</p>
            <p id="subjects-0biUwyjKkm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-0biUwyjKkm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0biUwyjKkm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0biUwyjKkm@OpenReview" onclick="foldPdfKimi('0biUwyjKkm@OpenReview', this)" class="hr hr-fold">
        </div><div id="koEALFNBj1@OpenReview" class="panel paper" keywords="textbf,reg,sit,repa,denoising,latents,256,times,image,martinser">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=koEALFNBj1" target="_blank" title="58/77"><span class="index notranslate">#58</span></a>
                <a id="title-koEALFNBj1@OpenReview" class="title-link" href="/venue/koEALFNBj1@OpenReview" target="_blank">Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think</a>
                <a id="pdf-koEALFNBj1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('koEALFNBj1@OpenReview', this)" data="https://openreview.net/pdf?id=koEALFNBj1">[PDF<sup id="pdf-stars-koEALFNBj1@OpenReview">20</sup>]</a>
                <a id="copy-koEALFNBj1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('koEALFNBj1@OpenReview')">[Copy]</a>
                <a id="kimi-koEALFNBj1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('koEALFNBj1@OpenReview', this)">[Kimi<sup id="kimi-stars-koEALFNBj1@OpenReview">9</sup>]</a>
                <a id="rel-koEALFNBj1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('koEALFNBj1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-koEALFNBj1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ge Wu" target="_blank">Ge Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shen Zhang" target="_blank">Shen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruijing Shi" target="_blank">Ruijing Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shanghua Gao" target="_blank">Shanghua Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyuan Chen" target="_blank">Zhenyuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Wang" target="_blank">Lei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaowei Chen" target="_blank">Zhaowei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongcheng Gao" target="_blank">Hongcheng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Tang" target="_blank">Yao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=jian Yang" target="_blank">jian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Ming Cheng" target="_blank">Ming-Ming Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Li" target="_blank">Xiang Li</a>
            </p>
            <p id="summary-koEALFNBj1@OpenReview" class="summary">REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called \textit{\textbf{R}epresentation \textbf{E}ntanglement for \textbf{G}eneration} (\textbf{REG}), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (&lt;0.5\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-76-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-530" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-531"><span class="mo" id="MathJax-Span-532" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-76">\times</script>256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-77-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;63&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-533" style="width: 2.346em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.77em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-534"><span class="texatom" id="MathJax-Span-535"><span class="mrow" id="MathJax-Span-536"><span class="mtext" id="MathJax-Span-537" style="font-family: MathJax_Main-bold;">63</span></span></span><span class="mo" id="MathJax-Span-538" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">63</mtext></mrow><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-77">\textbf{63}\times</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-78-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;23&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-539" style="width: 2.346em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.77em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-540"><span class="texatom" id="MathJax-Span-541"><span class="mrow" id="MathJax-Span-542"><span class="mtext" id="MathJax-Span-543" style="font-family: MathJax_Main-bold;">23</span></span></span><span class="mo" id="MathJax-Span-544" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">23</mtext></mrow><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-78">\textbf{23}\times</script> faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-79-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;10&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-545" style="width: 2.346em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.77em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-546"><span class="texatom" id="MathJax-Span-547"><span class="mrow" id="MathJax-Span-548"><span class="mtext" id="MathJax-Span-549" style="font-family: MathJax_Main-bold;">10</span></span></span><span class="mo" id="MathJax-Span-550" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">10</mtext></mrow><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-79">\textbf{10}\times</script> longer). Code is available at: https://github.com/Martinser/REG.</p>
            <p id="subjects-koEALFNBj1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-koEALFNBj1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-koEALFNBj1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-koEALFNBj1@OpenReview" onclick="foldPdfKimi('koEALFNBj1@OpenReview', this)" class="hr hr-fold">
        </div><div id="s6k9l5yX8e@OpenReview" class="panel paper" keywords="dynam3d,navigation,vln,language,vlm,dynamic,layered,environments,exploration,vision">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=s6k9l5yX8e" target="_blank" title="59/77"><span class="index notranslate">#59</span></a>
                <a id="title-s6k9l5yX8e@OpenReview" class="title-link" href="/venue/s6k9l5yX8e@OpenReview" target="_blank">Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation</a>
                <a id="pdf-s6k9l5yX8e@OpenReview" class="title-pdf notranslate" onclick="togglePdf('s6k9l5yX8e@OpenReview', this)" data="https://openreview.net/pdf?id=s6k9l5yX8e">[PDF<sup id="pdf-stars-s6k9l5yX8e@OpenReview">11</sup>]</a>
                <a id="copy-s6k9l5yX8e@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('s6k9l5yX8e@OpenReview')">[Copy]</a>
                <a id="kimi-s6k9l5yX8e@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('s6k9l5yX8e@OpenReview', this)">[Kimi<sup id="kimi-stars-s6k9l5yX8e@OpenReview">4</sup>]</a>
                <a id="rel-s6k9l5yX8e@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('s6k9l5yX8e@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-s6k9l5yX8e@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Wang" target="_blank">Zihan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungjun Lee" target="_blank">Seungjun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gim Hee Lee" target="_blank">Gim Hee Lee</a>
            </p>
            <p id="summary-s6k9l5yX8e@OpenReview" class="summary">Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.</p>
            <p id="subjects-s6k9l5yX8e@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-s6k9l5yX8e@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-s6k9l5yX8e@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-s6k9l5yX8e@OpenReview" onclick="foldPdfKimi('s6k9l5yX8e@OpenReview', this)" class="hr hr-fold">
        </div><div id="NM4emKloy6@OpenReview" class="panel paper" keywords="equivariance,equivariant,constrained,symmetries,optimization,strictly,gradually,data,smooths,departs">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NM4emKloy6" target="_blank" title="60/77"><span class="index notranslate">#60</span></a>
                <a id="title-NM4emKloy6@OpenReview" class="title-link" href="/venue/NM4emKloy6@OpenReview" target="_blank">Learning (Approximately) Equivariant Networks via Constrained Optimization</a>
                <a id="pdf-NM4emKloy6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NM4emKloy6@OpenReview', this)" data="https://openreview.net/pdf?id=NM4emKloy6">[PDF<sup id="pdf-stars-NM4emKloy6@OpenReview">9</sup>]</a>
                <a id="copy-NM4emKloy6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NM4emKloy6@OpenReview')">[Copy]</a>
                <a id="kimi-NM4emKloy6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NM4emKloy6@OpenReview', this)">[Kimi<sup id="kimi-stars-NM4emKloy6@OpenReview">3</sup>]</a>
                <a id="rel-NM4emKloy6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NM4emKloy6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NM4emKloy6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrei Manolache" target="_blank">Andrei Manolache</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luiz F. O. Chamon" target="_blank">Luiz F. O. Chamon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathias Niepert" target="_blank">Mathias Niepert</a>
            </p>
            <p id="summary-NM4emKloy6@OpenReview" class="summary">Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.</p>
            <p id="subjects-NM4emKloy6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-NM4emKloy6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NM4emKloy6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NM4emKloy6@OpenReview" onclick="foldPdfKimi('NM4emKloy6@OpenReview', this)" class="hr hr-fold">
        </div><div id="jRXgRC6fu7@OpenReview" class="panel paper" keywords="sage,state,objects,language,actions,recognition,action,graph,unified,unseen">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jRXgRC6fu7" target="_blank" title="61/77"><span class="index notranslate">#61</span></a>
                <a id="title-jRXgRC6fu7@OpenReview" class="title-link" href="/venue/jRXgRC6fu7@OpenReview" target="_blank">SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding</a>
                <a id="pdf-jRXgRC6fu7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jRXgRC6fu7@OpenReview', this)" data="https://openreview.net/pdf?id=jRXgRC6fu7">[PDF<sup id="pdf-stars-jRXgRC6fu7@OpenReview">9</sup>]</a>
                <a id="copy-jRXgRC6fu7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jRXgRC6fu7@OpenReview')">[Copy]</a>
                <a id="kimi-jRXgRC6fu7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jRXgRC6fu7@OpenReview', this)">[Kimi<sup id="kimi-stars-jRXgRC6fu7@OpenReview">3</sup>]</a>
                <a id="rel-jRXgRC6fu7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jRXgRC6fu7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jRXgRC6fu7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Zang" target="_blank">Yuan Zang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zitian Tang" target="_blank">Zitian Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junho Cho" target="_blank">Junho Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewook Yoo" target="_blank">Jaewook Yoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Sun" target="_blank">Chen Sun</a>
            </p>
            <p id="summary-jRXgRC6fu7@OpenReview" class="summary">Recognizing the physical states of objects and their transformations within videos is crucial for structured video understanding and enabling robust real-world applications, such as robotic manipulation. However, pretrained vision-language models often struggle to capture these nuanced dynamics and their temporal context, and specialized object state recognition frameworks may not generalize to unseen actions or objects. We introduce SAGE (State-Action Graph Embeddings), a novel framework that offers a unified model of physical state transitions by decomposing states into fine-grained, language-described visual concepts that are sharable across different objects and actions. SAGE initially leverages Large Language Models to construct a State-Action Graph, which is then multimodally refined using Vision-Language Models. Extensive experiments show that our method significantly outperforms baselines, generalizes effectively to unseen objects and actions in open-world settings. SAGE improves the prior state-of-the-art by as much as 14.6% on novel state recognition with less than 5% of its inference time.</p>
            <p id="subjects-jRXgRC6fu7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-jRXgRC6fu7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jRXgRC6fu7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jRXgRC6fu7@OpenReview" onclick="foldPdfKimi('jRXgRC6fu7@OpenReview', this)" class="hr hr-fold">
        </div><div id="4OsgYD7em5@OpenReview" class="panel paper" keywords="rlvr,reasoning,base,llms,textit,abilities,elicit,genuinely,models,incentivize">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4OsgYD7em5" target="_blank" title="62/77"><span class="index notranslate">#62</span></a>
                <a id="title-4OsgYD7em5@OpenReview" class="title-link" href="/venue/4OsgYD7em5@OpenReview" target="_blank">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a>
                <a id="pdf-4OsgYD7em5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4OsgYD7em5@OpenReview', this)" data="https://openreview.net/pdf?id=4OsgYD7em5">[PDF<sup id="pdf-stars-4OsgYD7em5@OpenReview">24</sup>]</a>
                <a id="copy-4OsgYD7em5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4OsgYD7em5@OpenReview')">[Copy]</a>
                <a id="kimi-4OsgYD7em5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4OsgYD7em5@OpenReview', this)">[Kimi<sup id="kimi-stars-4OsgYD7em5@OpenReview">24</sup>]</a>
                <a id="rel-4OsgYD7em5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4OsgYD7em5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4OsgYD7em5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Yue" target="_blank">Yang Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqi Chen" target="_blank">Zhiqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Lu" target="_blank">Rui Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Zhao" target="_blank">Andrew Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaokai Wang" target="_blank">Zhaokai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Yue" target="_blank">Yang Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiji Song" target="_blank">Shiji Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gao Huang" target="_blank">Gao Huang</a>
            </p>
            <p id="summary-4OsgYD7em5@OpenReview" class="summary">Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\textit{k} at large \textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-80-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-551" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-552"><span class="mi" id="MathJax-Span-553" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-80">k</script> (\eg, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-81-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-554" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-555"><span class="mi" id="MathJax-Span-556" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-81">k</script>=1), base models achieve higher pass@<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-82-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-557" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-558"><span class="mi" id="MathJax-Span-559" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-82">k</script> score when <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-83-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-560" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-561"><span class="mi" id="MathJax-Span-562" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-83">k</script> is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.</p>
            <p id="subjects-4OsgYD7em5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-4OsgYD7em5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4OsgYD7em5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4OsgYD7em5@OpenReview" onclick="foldPdfKimi('4OsgYD7em5@OpenReview', this)" class="hr hr-fold">
        </div><div id="s6YHno8Ke3@OpenReview" class="panel paper" keywords="meta,conml,contrastive,learners,learning,objective,training,identity,supervision,learn">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=s6YHno8Ke3" target="_blank" title="63/77"><span class="index notranslate">#63</span></a>
                <a id="title-s6YHno8Ke3@OpenReview" class="title-link" href="/venue/s6YHno8Ke3@OpenReview" target="_blank">Learning to Learn with Contrastive Meta-Objective</a>
                <a id="pdf-s6YHno8Ke3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('s6YHno8Ke3@OpenReview', this)" data="https://openreview.net/pdf?id=s6YHno8Ke3">[PDF<sup id="pdf-stars-s6YHno8Ke3@OpenReview">7</sup>]</a>
                <a id="copy-s6YHno8Ke3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('s6YHno8Ke3@OpenReview')">[Copy]</a>
                <a id="kimi-s6YHno8Ke3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('s6YHno8Ke3@OpenReview', this)">[Kimi<sup id="kimi-stars-s6YHno8Ke3@OpenReview">8</sup>]</a>
                <a id="rel-s6YHno8Ke3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('s6YHno8Ke3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-s6YHno8Ke3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiguang Wu" target="_blank">Shiguang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaqing Wang" target="_blank">Yaqing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yatao Bian" target="_blank">Yatao Bian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanming Yao" target="_blank">Quanming Yao</a>
            </p>
            <p id="summary-s6YHno8Ke3@OpenReview" class="summary">Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost.</p>
            <p id="subjects-s6YHno8Ke3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-s6YHno8Ke3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-s6YHno8Ke3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-s6YHno8Ke3@OpenReview" onclick="foldPdfKimi('s6YHno8Ke3@OpenReview', this)" class="hr hr-fold">
        </div><div id="JFygzwx8SJ@OpenReview" class="panel paper" keywords="kvzip,cache,query,eviction,context,agnostic,pairs,latency,evicting,qwen2">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JFygzwx8SJ" target="_blank" title="64/77"><span class="index notranslate">#64</span></a>
                <a id="title-JFygzwx8SJ@OpenReview" class="title-link" href="/venue/JFygzwx8SJ@OpenReview" target="_blank">KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</a>
                <a id="pdf-JFygzwx8SJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JFygzwx8SJ@OpenReview', this)" data="https://openreview.net/pdf?id=JFygzwx8SJ">[PDF<sup id="pdf-stars-JFygzwx8SJ@OpenReview">17</sup>]</a>
                <a id="copy-JFygzwx8SJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JFygzwx8SJ@OpenReview')">[Copy]</a>
                <a id="kimi-JFygzwx8SJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JFygzwx8SJ@OpenReview', this)">[Kimi<sup id="kimi-stars-JFygzwx8SJ@OpenReview">10</sup>]</a>
                <a id="rel-JFygzwx8SJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JFygzwx8SJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JFygzwx8SJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jang-Hyun Kim" target="_blank">Jang-Hyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinuk Kim" target="_blank">Jinuk Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangwoo Kwon" target="_blank">Sangwoo Kwon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae W. Lee" target="_blank">Jae W. Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangdoo Yun" target="_blank">Sangdoo Yun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyun Oh Song" target="_blank">Hyun Oh Song</a>
            </p>
            <p id="summary-JFygzwx8SJ@OpenReview" class="summary">Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-84-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-563" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-564"><span class="mn" id="MathJax-Span-565" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-84">3</script>-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-85-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-566" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-567"><span class="mn" id="MathJax-Span-568" style="font-family: MathJax_Main;">4</span><span class="mo" id="MathJax-Span-569" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-85">4\times</script> and FlashAttention decoding latency by approximately <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-86-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-570" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-571"><span class="mn" id="MathJax-Span-572" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-573" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-86">2\times</script>, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\% cache budget ratio under multi-query scenarios.</p>
            <p id="subjects-JFygzwx8SJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-JFygzwx8SJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JFygzwx8SJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JFygzwx8SJ@OpenReview" onclick="foldPdfKimi('JFygzwx8SJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="NM8Apk61NA@OpenReview" class="panel paper" keywords="granularity,mllms,hyperbolic,alg,matrices,textual,modal,hyperet,levels,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NM8Apk61NA" target="_blank" title="65/77"><span class="index notranslate">#65</span></a>
                <a id="title-NM8Apk61NA@OpenReview" class="title-link" href="/venue/NM8Apk61NA@OpenReview" target="_blank">HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</a>
                <a id="pdf-NM8Apk61NA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NM8Apk61NA@OpenReview', this)" data="https://openreview.net/pdf?id=NM8Apk61NA">[PDF<sup id="pdf-stars-NM8Apk61NA@OpenReview">17</sup>]</a>
                <a id="copy-NM8Apk61NA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NM8Apk61NA@OpenReview')">[Copy]</a>
                <a id="kimi-NM8Apk61NA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NM8Apk61NA@OpenReview', this)">[Kimi<sup id="kimi-stars-NM8Apk61NA@OpenReview">13</sup>]</a>
                <a id="rel-NM8Apk61NA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NM8Apk61NA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NM8Apk61NA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zelin Peng" target="_blank">Zelin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqin Xu" target="_blank">Zhengqin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyang Liu" target="_blank">Qingyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaokang Yang" target="_blank">Xiaokang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Shen" target="_blank">Wei Shen</a>
            </p>
            <p id="summary-NM8Apk61NA@OpenReview" class="summary">Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \alg employs learnable matrices with Möbius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\% additional parameters.</p>
            <p id="subjects-NM8Apk61NA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-NM8Apk61NA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NM8Apk61NA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NM8Apk61NA@OpenReview" onclick="foldPdfKimi('NM8Apk61NA@OpenReview', this)" class="hr hr-fold">
        </div><div id="zwCb9cKHpd@OpenReview" class="panel paper" keywords="savvy,audio,spatial,llms,visual,reasoning,dynamic,bench,map,queried">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zwCb9cKHpd" target="_blank" title="66/77"><span class="index notranslate">#66</span></a>
                <a id="title-zwCb9cKHpd@OpenReview" class="title-link" href="/venue/zwCb9cKHpd@OpenReview" target="_blank">SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing</a>
                <a id="pdf-zwCb9cKHpd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zwCb9cKHpd@OpenReview', this)" data="https://openreview.net/pdf?id=zwCb9cKHpd">[PDF<sup id="pdf-stars-zwCb9cKHpd@OpenReview">9</sup>]</a>
                <a id="copy-zwCb9cKHpd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zwCb9cKHpd@OpenReview')">[Copy]</a>
                <a id="kimi-zwCb9cKHpd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zwCb9cKHpd@OpenReview', this)">[Kimi<sup id="kimi-stars-zwCb9cKHpd@OpenReview">5</sup>]</a>
                <a id="rel-zwCb9cKHpd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zwCb9cKHpd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zwCb9cKHpd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mingfei Chen" target="_blank">Mingfei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijun Cui" target="_blank">Zijun Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiulong Liu" target="_blank">Xiulong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlin Xiang" target="_blank">Jinlin Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caleb Zheng" target="_blank">Caleb Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyuan Li" target="_blank">Jingyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eli Shlizerman" target="_blank">Eli Shlizerman</a>
            </p>
            <p id="summary-zwCb9cKHpd@OpenReview" class="summary">3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of carefully curated question–answer pairs probing both directional and distance relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.</p>
            <p id="subjects-zwCb9cKHpd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-zwCb9cKHpd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zwCb9cKHpd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zwCb9cKHpd@OpenReview" onclick="foldPdfKimi('zwCb9cKHpd@OpenReview', this)" class="hr hr-fold">
        </div><div id="WCRPgBpbcA@OpenReview" class="panel paper" keywords="collapses,multiscale,moderate,regime,tokens,mean,interaction,clusters,exemplifying,transformers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WCRPgBpbcA" target="_blank" title="67/77"><span class="index notranslate">#67</span></a>
                <a id="title-WCRPgBpbcA@OpenReview" class="title-link" href="/venue/WCRPgBpbcA@OpenReview" target="_blank">A multiscale analysis of mean-field transformers in the moderate interaction regime</a>
                <a id="pdf-WCRPgBpbcA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WCRPgBpbcA@OpenReview', this)" data="https://openreview.net/pdf?id=WCRPgBpbcA">[PDF<sup id="pdf-stars-WCRPgBpbcA@OpenReview">6</sup>]</a>
                <a id="copy-WCRPgBpbcA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WCRPgBpbcA@OpenReview')">[Copy]</a>
                <a id="kimi-WCRPgBpbcA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WCRPgBpbcA@OpenReview', this)">[Kimi<sup id="kimi-stars-WCRPgBpbcA@OpenReview">4</sup>]</a>
                <a id="rel-WCRPgBpbcA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WCRPgBpbcA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WCRPgBpbcA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giuseppe Bruno" target="_blank">Giuseppe Bruno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Pasqualotto" target="_blank">Federico Pasqualotto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Agazzi" target="_blank">Andrea Agazzi</a>
            </p>
            <p id="summary-WCRPgBpbcA@OpenReview" class="summary">In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-87-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-574" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-575"><span class="mi" id="MathJax-Span-576" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-87">N</script> of tokens is large and the inverse temperature parameter <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-88-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-577" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-578"><span class="mi" id="MathJax-Span-579" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></span></span><script type="math/tex" id="MathJax-Element-88">\beta</script> of the model scales together with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-89-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-580" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-581"><span class="mi" id="MathJax-Span-582" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-89">N</script>. In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations.</p>
            <p id="subjects-WCRPgBpbcA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-WCRPgBpbcA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WCRPgBpbcA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WCRPgBpbcA@OpenReview" onclick="foldPdfKimi('WCRPgBpbcA@OpenReview', this)" class="hr hr-fold">
        </div><div id="CaSQgef484@OpenReview" class="panel paper" keywords="grafting,designs,dit,dits,attention,pretrained,quality,fid,depth,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CaSQgef484" target="_blank" title="68/77"><span class="index notranslate">#68</span></a>
                <a id="title-CaSQgef484@OpenReview" class="title-link" href="/venue/CaSQgef484@OpenReview" target="_blank">Exploring Diffusion Transformer Designs via Grafting</a>
                <a id="pdf-CaSQgef484@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CaSQgef484@OpenReview', this)" data="https://openreview.net/pdf?id=CaSQgef484">[PDF<sup id="pdf-stars-CaSQgef484@OpenReview">5</sup>]</a>
                <a id="copy-CaSQgef484@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CaSQgef484@OpenReview')">[Copy]</a>
                <a id="kimi-CaSQgef484@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CaSQgef484@OpenReview', this)">[Kimi<sup id="kimi-stars-CaSQgef484@OpenReview">8</sup>]</a>
                <a id="rel-CaSQgef484@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CaSQgef484@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CaSQgef484@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Keshigeyan Chandrasegaran" target="_blank">Keshigeyan Chandrasegaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Poli" target="_blank">Michael Poli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Y Fu" target="_blank">Daniel Y Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongjun Kim" target="_blank">Dongjun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lea M. Hadzic" target="_blank">Lea M. Hadzic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manling Li" target="_blank">Manling Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Agrim Gupta" target="_blank">Agrim Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Massaroli" target="_blank">Stefano Massaroli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azalia Mirhoseini" target="_blank">Azalia Mirhoseini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Carlos Niebles" target="_blank">Juan Carlos Niebles</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Ermon" target="_blank">Stefano Ermon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Fei-Fei" target="_blank">Li Fei-Fei</a>
            </p>
            <p id="summary-CaSQgef484@OpenReview" class="summary">Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present *grafting*, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs. 2.27 for DiT-XL/2) using <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-90-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-583" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.51em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-584"><span class="mo" id="MathJax-Span-585" style="font-family: MathJax_Main;">&lt;</span><span class="mn" id="MathJax-Span-586" style="font-family: MathJax_Main; padding-left: 0.263em;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-90"><2</script>% pretraining compute. We then graft a text-to-image model (PixArt-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-91-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A3;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-587" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-588"><span class="mi" id="MathJax-Span-589" style="font-family: MathJax_Main;">Σ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Σ</mi></math></span></span><script type="math/tex" id="MathJax-Element-91">\Sigma</script>), achieving a 1.43<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-92-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-590" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-591"><span class="mo" id="MathJax-Span-592" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-92">\times</script> speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-93-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-593" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-594"><span class="mo" id="MathJax-Span-595" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-93">\times</script> and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu.</p>
            <p id="subjects-CaSQgef484@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-CaSQgef484@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CaSQgef484@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CaSQgef484@OpenReview" onclick="foldPdfKimi('CaSQgef484@OpenReview', this)" class="hr hr-fold">
        </div><div id="rMdf8jhLR7@OpenReview" class="panel paper" keywords="clipping,gradient,smoothness,scion,norm,euclidean,descent,generalized,epfl,clippedscion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rMdf8jhLR7" target="_blank" title="69/77"><span class="index notranslate">#69</span></a>
                <a id="title-rMdf8jhLR7@OpenReview" class="title-link" href="/venue/rMdf8jhLR7@OpenReview" target="_blank">Generalized Gradient Norm Clipping &amp; Non-Euclidean <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-94-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-596" style="width: 4.168em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.474em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.321em, 1003.37em, 2.536em, -999.998em); top: -2.186em; left: 0em;"><span class="mrow" id="MathJax-Span-597"><span class="mo" id="MathJax-Span-598" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-599"><span style="display: inline-block; position: relative; width: 1.113em; height: 0px;"><span style="position: absolute; clip: rect(1.356em, 1000.66em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mi" id="MathJax-Span-600" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.696em;"><span class="mn" id="MathJax-Span-601" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span><span class="mo" id="MathJax-Span-602" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-603" style="padding-left: 0.175em;"><span style="display: inline-block; position: relative; width: 1.113em; height: 0px;"><span style="position: absolute; clip: rect(1.356em, 1000.66em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mi" id="MathJax-Span-604" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.696em;"><span class="mn" id="MathJax-Span-605" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span><span class="mo" id="MathJax-Span-606" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.189em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.294em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>L</mi><mn>0</mn></msub><mo>,</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-94">(L_0,L_1)</script>-Smoothness</a>
                <a id="pdf-rMdf8jhLR7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rMdf8jhLR7@OpenReview', this)" data="https://openreview.net/pdf?id=rMdf8jhLR7">[PDF<sup id="pdf-stars-rMdf8jhLR7@OpenReview">5</sup>]</a>
                <a id="copy-rMdf8jhLR7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rMdf8jhLR7@OpenReview')">[Copy]</a>
                <a id="kimi-rMdf8jhLR7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rMdf8jhLR7@OpenReview', this)">[Kimi<sup id="kimi-stars-rMdf8jhLR7@OpenReview">10</sup>]</a>
                <a id="rel-rMdf8jhLR7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rMdf8jhLR7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rMdf8jhLR7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Pethick" target="_blank">Thomas Pethick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanyun Xie" target="_blank">Wanyun Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mete Erdogan" target="_blank">Mete Erdogan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kimon Antonakopoulos" target="_blank">Kimon Antonakopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tony Silveti-Falls" target="_blank">Tony Silveti-Falls</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Volkan Cevher" target="_blank">Volkan Cevher</a>
            </p>
            <p id="summary-rMdf8jhLR7@OpenReview" class="summary">This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-95-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-607" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-608"><span class="msubsup" id="MathJax-Span-609"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-610" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.68em;"><span class="mn" id="MathJax-Span-611" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-95">L_0</script>,<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-96-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-612" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-613"><span class="msubsup" id="MathJax-Span-614"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-615" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.68em;"><span class="mn" id="MathJax-Span-616" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-96">L_1</script>)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-97-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-617" style="width: 4.638em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.75em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-618"><span class="mi" id="MathJax-Span-619" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-620" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-621"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-622" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-623"><span class="mrow" id="MathJax-Span-624"><span class="mo" id="MathJax-Span-625" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-626" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-627"><span class="mrow" id="MathJax-Span-628"><span class="mo" id="MathJax-Span-629" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-630" style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-631" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-97">O(n^{-1/4})</script> convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion.</p>
            <p id="subjects-rMdf8jhLR7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-rMdf8jhLR7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rMdf8jhLR7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rMdf8jhLR7@OpenReview" onclick="foldPdfKimi('rMdf8jhLR7@OpenReview', this)" class="hr hr-fold">
        </div><div id="Q6IyUpBmrG@OpenReview" class="panel paper" keywords="multimodal,disproportion,classification,ability,modalities,boosting,mitigating,imbalance,njustkmg,learning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Q6IyUpBmrG" target="_blank" title="70/77"><span class="index notranslate">#70</span></a>
                <a id="title-Q6IyUpBmrG@OpenReview" class="title-link" href="/venue/Q6IyUpBmrG@OpenReview" target="_blank">Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion</a>
                <a id="pdf-Q6IyUpBmrG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Q6IyUpBmrG@OpenReview', this)" data="https://openreview.net/pdf?id=Q6IyUpBmrG">[PDF<sup id="pdf-stars-Q6IyUpBmrG@OpenReview">11</sup>]</a>
                <a id="copy-Q6IyUpBmrG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Q6IyUpBmrG@OpenReview')">[Copy]</a>
                <a id="kimi-Q6IyUpBmrG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Q6IyUpBmrG@OpenReview', this)">[Kimi<sup id="kimi-stars-Q6IyUpBmrG@OpenReview">8</sup>]</a>
                <a id="rel-Q6IyUpBmrG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Q6IyUpBmrG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Q6IyUpBmrG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qing-Yuan Jiang" target="_blank">Qing-Yuan Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longfei Huang" target="_blank">Longfei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Yang" target="_blank">Yang Yang</a>
            </p>
            <p id="summary-Q6IyUpBmrG@OpenReview" class="summary">Multimodal learning (MML) is significantly constrained by modality imbalance, leading to suboptimal performance in practice. While existing approaches primarily focus on balancing the learning of different modalities to address this issue, they fundamentally overlook the inherent disproportion in model classification ability, which serves as the primary cause of this phenomenon. In this paper, we propose a novel multimodal learning approach to dynamically balance the classification ability of weak and strong modalities by incorporating the principle of boosting. Concretely, we first propose a sustained boosting algorithm in multimodal learning by simultaneously optimizing the classification and residual errors. Subsequently, we introduce an adaptive classifier assignment strategy to dynamically facilitate the classification performance of the weak modality. Furthermore, we theoretically analyze the convergence property of the cross-modal gap function, ensuring the effectiveness of the proposed boosting scheme. To this end, the classification ability of strong and weak modalities is expected to be balanced, thereby mitigating the imbalance issue. Empirical experiments on widely used datasets reveal the superiority of our method through comparison with various state-of-the-art (SOTA) multimodal learning baselines. The source code is available at https://github.com/njustkmg/NeurIPS25-AUG.</p>
            <p id="subjects-Q6IyUpBmrG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-Q6IyUpBmrG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Q6IyUpBmrG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Q6IyUpBmrG@OpenReview" onclick="foldPdfKimi('Q6IyUpBmrG@OpenReview', this)" class="hr hr-fold">
        </div><div id="XQ87Vo9GIz@OpenReview" class="panel paper" keywords="transfertraj,transferability,region,task,vehicle,transfer,trajectory,retraining,tasks,spatial">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XQ87Vo9GIz" target="_blank" title="71/77"><span class="index notranslate">#71</span></a>
                <a id="title-XQ87Vo9GIz@OpenReview" class="title-link" href="/venue/XQ87Vo9GIz@OpenReview" target="_blank">TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability</a>
                <a id="pdf-XQ87Vo9GIz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XQ87Vo9GIz@OpenReview', this)" data="https://openreview.net/pdf?id=XQ87Vo9GIz">[PDF<sup id="pdf-stars-XQ87Vo9GIz@OpenReview">9</sup>]</a>
                <a id="copy-XQ87Vo9GIz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XQ87Vo9GIz@OpenReview')">[Copy]</a>
                <a id="kimi-XQ87Vo9GIz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XQ87Vo9GIz@OpenReview', this)">[Kimi<sup id="kimi-stars-XQ87Vo9GIz@OpenReview">14</sup>]</a>
                <a id="rel-XQ87Vo9GIz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XQ87Vo9GIz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XQ87Vo9GIz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tonglong Wei" target="_blank">Tonglong Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Lin" target="_blank">Yan Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Zhou" target="_blank">Zeyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haomin Wen" target="_blank">Haomin Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jilin Hu" target="_blank">Jilin Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengnan Guo" target="_blank">Shengnan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youfang Lin" target="_blank">Youfang Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gao Cong" target="_blank">Gao Cong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaiyu Wan" target="_blank">Huaiyu Wan</a>
            </p>
            <p id="summary-XQ87Vo9GIz@OpenReview" class="summary">Vehicle GPS trajectories provide valuable movement information that supports various downstream tasks and applications. A desirable trajectory learning model should be able to transfer across regions and tasks without retraining, avoiding the need to maintain multiple specialized models and subpar performance with limited training data. However, each region has its unique spatial features and contexts, which are reflected in vehicle movement patterns and are difficult to generalize. Additionally, transferring across different tasks faces technical challenges due to the varying input-output structures required for each task. Existing efforts towards transferability primarily involve learning embedding vectors for trajectories, which perform poorly in region transfer and require retraining of prediction modules for task transfer. To address these challenges, we propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-98-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;TransferTraj&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-632" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.84em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-633"><span class="texatom" id="MathJax-Span-634"><span class="mrow" id="MathJax-Span-635"><span class="mtext" id="MathJax-Span-636" style="font-family: MathJax_Main-italic;">TransferTraj</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">TransferTraj</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-98">\textit{TransferTraj}</script>, a vehicle GPS trajectory learning model that excels in both region and task transferability. For region transferability, we introduce RTTE as the main learnable module within TransferTraj. It integrates spatial, temporal, POI, and road network modalities of trajectories to effectively manage variations in spatial context distribution across regions. It also introduces a TRIE module for incorporating relative information of spatial features and a spatial context MoE module for handling movement patterns in diverse contexts. For task transferability, we propose a task-transferable input-output scheme that unifies the input-output structure of different tasks into the masking and recovery of modalities and trajectory points. This approach allows TransferTraj to be pre-trained once and transferred to different tasks without retraining. We conduct extensive experiments on three real-world vehicle trajectory datasets under various transfer settings, including task transfer, zero-shot region transfer, and few-shot region transfer. Experimental results demonstrate that TransferTraj significantly outperforms state-of-the-art baselines in different scenarios, validating its effectiveness in region and task transfer. Code is available at https://github.com/wtl52656/TransferTraj.</p>
            <p id="subjects-XQ87Vo9GIz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-XQ87Vo9GIz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XQ87Vo9GIz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XQ87Vo9GIz@OpenReview" onclick="foldPdfKimi('XQ87Vo9GIz@OpenReview', this)" class="hr hr-fold">
        </div><div id="zIzZxDsNNP@OpenReview" class="panel paper" keywords="physense,placements,sensor,placement,sensing,physics,sparse,reconstruction,stage,thuml">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zIzZxDsNNP" target="_blank" title="72/77"><span class="index notranslate">#72</span></a>
                <a id="title-zIzZxDsNNP@OpenReview" class="title-link" href="/venue/zIzZxDsNNP@OpenReview" target="_blank">PhySense: Sensor Placement Optimization for Accurate Physics Sensing</a>
                <a id="pdf-zIzZxDsNNP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zIzZxDsNNP@OpenReview', this)" data="https://openreview.net/pdf?id=zIzZxDsNNP">[PDF<sup id="pdf-stars-zIzZxDsNNP@OpenReview">9</sup>]</a>
                <a id="copy-zIzZxDsNNP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zIzZxDsNNP@OpenReview')">[Copy]</a>
                <a id="kimi-zIzZxDsNNP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zIzZxDsNNP@OpenReview', this)">[Kimi<sup id="kimi-stars-zIzZxDsNNP@OpenReview">7</sup>]</a>
                <a id="rel-zIzZxDsNNP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zIzZxDsNNP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zIzZxDsNNP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuezhou Ma" target="_blank">Yuezhou Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haixu Wu" target="_blank">Haixu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Zhou" target="_blank">Hang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huikun Weng" target="_blank">Huikun Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianmin Wang" target="_blank">Jianmin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingsheng Long" target="_blank">Mingsheng Long</a>
            </p>
            <p id="summary-zIzZxDsNNP@OpenReview" class="summary">Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense.</p>
            <p id="subjects-zIzZxDsNNP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-zIzZxDsNNP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zIzZxDsNNP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zIzZxDsNNP@OpenReview" onclick="foldPdfKimi('zIzZxDsNNP@OpenReview', this)" class="hr hr-fold">
        </div><div id="JcEqp4aPmb@OpenReview" class="panel paper" keywords="infinitystar,video,autoregressive,720p,uniﬁed,generation,vbench,spacetime,modeling,unified">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JcEqp4aPmb" target="_blank" title="73/77"><span class="index notranslate">#73</span></a>
                <a id="title-JcEqp4aPmb@OpenReview" class="title-link" href="/venue/JcEqp4aPmb@OpenReview" target="_blank">InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation</a>
                <a id="pdf-JcEqp4aPmb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JcEqp4aPmb@OpenReview', this)" data="https://openreview.net/pdf?id=JcEqp4aPmb">[PDF<sup id="pdf-stars-JcEqp4aPmb@OpenReview">18</sup>]</a>
                <a id="copy-JcEqp4aPmb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JcEqp4aPmb@OpenReview')">[Copy]</a>
                <a id="kimi-JcEqp4aPmb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JcEqp4aPmb@OpenReview', this)">[Kimi<sup id="kimi-stars-JcEqp4aPmb@OpenReview">11</sup>]</a>
                <a id="rel-JcEqp4aPmb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JcEqp4aPmb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JcEqp4aPmb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlai Liu" target="_blank">Jinlai Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Han" target="_blank">Jian Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Yan" target="_blank">Bin Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wuhui" target="_blank">Wuhui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengda Zhu" target="_blank">Fengda Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Wang" target="_blank">Xing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Jiang" target="_blank">Yi Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=BINGYUE PENG" target="_blank">BINGYUE PENG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zehuan Yuan" target="_blank">Zehuan Yuan</a>
            </p>
            <p id="summary-JcEqp4aPmb@OpenReview" class="summary">We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long-duration video synthesis via straightforward temporal autoregression. Through extensive experiments, InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-99-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-637" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-638"><span class="mo" id="MathJax-Span-639" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-99">\times</script> faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.</p>
            <p id="subjects-JcEqp4aPmb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-JcEqp4aPmb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JcEqp4aPmb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JcEqp4aPmb@OpenReview" onclick="foldPdfKimi('JcEqp4aPmb@OpenReview', this)" class="hr hr-fold">
        </div><div id="gL4muAFwsh@OpenReview" class="panel paper" keywords="regret,sgb,bandits,logarithmic,gradient,bandit,armed,succeed,learning,rate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gL4muAFwsh" target="_blank" title="74/77"><span class="index notranslate">#74</span></a>
                <a id="title-gL4muAFwsh@OpenReview" class="title-link" href="/venue/gL4muAFwsh@OpenReview" target="_blank">Does Stochastic Gradient really succeed for bandits?</a>
                <a id="pdf-gL4muAFwsh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gL4muAFwsh@OpenReview', this)" data="https://openreview.net/pdf?id=gL4muAFwsh">[PDF<sup id="pdf-stars-gL4muAFwsh@OpenReview">7</sup>]</a>
                <a id="copy-gL4muAFwsh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gL4muAFwsh@OpenReview')">[Copy]</a>
                <a id="kimi-gL4muAFwsh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gL4muAFwsh@OpenReview', this)">[Kimi<sup id="kimi-stars-gL4muAFwsh@OpenReview">6</sup>]</a>
                <a id="rel-gL4muAFwsh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gL4muAFwsh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gL4muAFwsh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dorian Baudry" target="_blank">Dorian Baudry</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emmeran Johnson" target="_blank">Emmeran Johnson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Vary" target="_blank">Simon Vary</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ciara Pike-Burke" target="_blank">Ciara Pike-Burke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick Rebeschini" target="_blank">Patrick Rebeschini</a>
            </p>
            <p id="summary-gL4muAFwsh@OpenReview" class="summary">Recent works of Mei et al. (2023, 2024) have deepened the theoretical understanding of the *Stochastic Gradient Bandit* (SGB) policy, showing that using a constant learning rate guarantees asymptotic convergence to the optimal policy, and that sufficiently *small* learning rates can yield logarithmic regret. However, whether logarithmic regret holds beyond small learning rates remains unclear. In this work, we take a step towards characterizing the regret *regimes* of SGB as a function of its learning rate. For two--armed bandits, we identify a sharp threshold, scaling with the sub-optimality gap <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-100-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-640" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-641"><span class="mi" id="MathJax-Span-642" style="font-family: MathJax_Main;">Δ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi></math></span></span><script type="math/tex" id="MathJax-Element-100">\Delta</script>, below which SGB achieves *logarithmic* regret on all instances, and above which it can incur *polynomial* regret on some instances. This result highlights the necessity of knowing (or estimating) <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-101-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-643" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-644"><span class="mi" id="MathJax-Span-645" style="font-family: MathJax_Main;">Δ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi></math></span></span><script type="math/tex" id="MathJax-Element-101">\Delta</script> to ensure logarithmic regret with a constant learning rate. For general <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-102-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-646" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-647"><span class="mi" id="MathJax-Span-648" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></span></span><script type="math/tex" id="MathJax-Element-102">K</script>-armed bandits, we further show the learning rate must scale inversely with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-103-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-649" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-650"><span class="mi" id="MathJax-Span-651" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></span></span><script type="math/tex" id="MathJax-Element-103">K</script> to avoid polynomial regret. We introduce novel techniques to derive regret upper bounds for SGB, laying the groundwork for future advances in the theory of gradient-based bandit algorithms.</p>
            <p id="subjects-gL4muAFwsh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-gL4muAFwsh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gL4muAFwsh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gL4muAFwsh@OpenReview" onclick="foldPdfKimi('gL4muAFwsh@OpenReview', this)" class="hr hr-fold">
        </div><div id="INqBOmwIpG@OpenReview" class="panel paper" keywords="perception,video,embeddings,tasks,vision,alignment,encoders,pretraining,encoder,image">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=INqBOmwIpG" target="_blank" title="75/77"><span class="index notranslate">#75</span></a>
                <a id="title-INqBOmwIpG@OpenReview" class="title-link" href="/venue/INqBOmwIpG@OpenReview" target="_blank">Perception Encoder: The best visual embeddings are not at the output of the network</a>
                <a id="pdf-INqBOmwIpG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('INqBOmwIpG@OpenReview', this)" data="https://openreview.net/pdf?id=INqBOmwIpG">[PDF<sup id="pdf-stars-INqBOmwIpG@OpenReview">25</sup>]</a>
                <a id="copy-INqBOmwIpG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('INqBOmwIpG@OpenReview')">[Copy]</a>
                <a id="kimi-INqBOmwIpG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('INqBOmwIpG@OpenReview', this)">[Kimi<sup id="kimi-stars-INqBOmwIpG@OpenReview">13</sup>]</a>
                <a id="rel-INqBOmwIpG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('INqBOmwIpG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-INqBOmwIpG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Bolya" target="_blank">Daniel Bolya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Po-Yao Huang" target="_blank">Po-Yao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peize Sun" target="_blank">Peize Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jang Hyun Cho" target="_blank">Jang Hyun Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Madotto" target="_blank">Andrea Madotto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Wei" target="_blank">Chen Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengyu Ma" target="_blank">Tengyu Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiale Zhi" target="_blank">Jiale Zhi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jathushan Rajasegaran" target="_blank">Jathushan Rajasegaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanoona Abdul Rasheed" target="_blank">Hanoona Abdul Rasheed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junke Wang" target="_blank">Junke Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Monteiro" target="_blank">Marco Monteiro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hu Xu" target="_blank">Hu Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Dong" target="_blank">Shiyu Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikhila Ravi" target="_blank">Nikhila Ravi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shang-Wen Li" target="_blank">Shang-Wen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piotr Dollar" target="_blank">Piotr Dollar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Feichtenhofer" target="_blank">Christoph Feichtenhofer</a>
            </p>
            <p id="summary-INqBOmwIpG@OpenReview" class="summary">We introduce Perception Encoder (PE), a family of state-of-the-art vision encoders for image and video understanding. Traditionally, vision encoders have relied on a variety of pretraining objectives, each excelling at different downstream tasks. Surprisingly, after scaling a carefully tuned image pretraining recipe and refining with a robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves state-of-the-art results on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&amp;A; and spatial tasks such as detection, tracking, and depth estimation. We release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models</p>
            <p id="subjects-INqBOmwIpG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-INqBOmwIpG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-INqBOmwIpG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-INqBOmwIpG@OpenReview" onclick="foldPdfKimi('INqBOmwIpG@OpenReview', this)" class="hr hr-fold">
        </div><div id="Gq4Gay8rDB@OpenReview" class="panel paper" keywords="egocentric,playerone,world,video,exocentric,scene,simulator,movements,motion,coarse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Gq4Gay8rDB" target="_blank" title="76/77"><span class="index notranslate">#76</span></a>
                <a id="title-Gq4Gay8rDB@OpenReview" class="title-link" href="/venue/Gq4Gay8rDB@OpenReview" target="_blank">PlayerOne: Egocentric World Simulator</a>
                <a id="pdf-Gq4Gay8rDB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Gq4Gay8rDB@OpenReview', this)" data="https://openreview.net/pdf?id=Gq4Gay8rDB">[PDF<sup id="pdf-stars-Gq4Gay8rDB@OpenReview">17</sup>]</a>
                <a id="copy-Gq4Gay8rDB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Gq4Gay8rDB@OpenReview')">[Copy]</a>
                <a id="kimi-Gq4Gay8rDB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Gq4Gay8rDB@OpenReview', this)">[Kimi<sup id="kimi-stars-Gq4Gay8rDB@OpenReview">9</sup>]</a>
                <a id="rel-Gq4Gay8rDB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Gq4Gay8rDB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Gq4Gay8rDB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanpeng Tu" target="_blank">Yuanpeng Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Luo" target="_blank">Hao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Chen" target="_blank">Xi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Bai" target="_blank">Xiang Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Wang" target="_blank">Fan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengshuang Zhao" target="_blank">Hengshuang Zhao</a>
            </p>
            <p id="summary-Gq4Gay8rDB@OpenReview" class="summary">We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real-scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and world-consistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.</p>
            <p id="subjects-Gq4Gay8rDB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-Gq4Gay8rDB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gq4Gay8rDB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gq4Gay8rDB@OpenReview" onclick="foldPdfKimi('Gq4Gay8rDB@OpenReview', this)" class="hr hr-fold">
        </div><div id="uWj4s7rMnR@OpenReview" class="panel paper" keywords="meanflow,step,flow,256,generative,instantaneous,nfe,modeling,one,narrows">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uWj4s7rMnR" target="_blank" title="77/77"><span class="index notranslate">#77</span></a>
                <a id="title-uWj4s7rMnR@OpenReview" class="title-link" href="/venue/uWj4s7rMnR@OpenReview" target="_blank">Mean Flows for One-step Generative Modeling</a>
                <a id="pdf-uWj4s7rMnR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uWj4s7rMnR@OpenReview', this)" data="https://openreview.net/pdf?id=uWj4s7rMnR">[PDF<sup id="pdf-stars-uWj4s7rMnR@OpenReview">27</sup>]</a>
                <a id="copy-uWj4s7rMnR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uWj4s7rMnR@OpenReview')">[Copy]</a>
                <a id="kimi-uWj4s7rMnR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uWj4s7rMnR@OpenReview', this)">[Kimi<sup id="kimi-stars-uWj4s7rMnR@OpenReview">14</sup>]</a>
                <a id="rel-uWj4s7rMnR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uWj4s7rMnR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uWj4s7rMnR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyang Geng" target="_blank">Zhengyang Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyang Deng" target="_blank">Mingyang Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjian Bai" target="_blank">Xingjian Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=J Zico Kolter" target="_blank">J Zico Kolter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiming He" target="_blank">Kaiming He</a>
            </p>
            <p id="summary-uWj4s7rMnR@OpenReview" class="summary">We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-104-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-652" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-653"><span class="mo" id="MathJax-Span-654" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-104">\times</script>256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.</p>
            <p id="subjects-uWj4s7rMnR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/NeurIPS.2025?group=Oral" target="_blank">NeurIPS.2025 - Oral</a>
            </p>
            <div id="pdf-container-uWj4s7rMnR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uWj4s7rMnR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uWj4s7rMnR@OpenReview" onclick="foldPdfKimi('uWj4s7rMnR@OpenReview', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-KurYdcCbjv@OpenReview" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#KurYdcCbjv@OpenReview">Generalized Linear Mode Connectivity for Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('KurYdcCbjv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KurYdcCbjv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-jzPQRbGkAq@OpenReview" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#jzPQRbGkAq@OpenReview">Deep Compositional Phase Diffusion for Long Motion Sequence Generation</a>
                    <a class="i-star" onclick="toggleAppStar('jzPQRbGkAq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jzPQRbGkAq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-eafIjoZAHm@OpenReview" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#eafIjoZAHm@OpenReview">GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability</a>
                    <a class="i-star" onclick="toggleAppStar('eafIjoZAHm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eafIjoZAHm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-tirl2l9oKg@OpenReview" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#tirl2l9oKg@OpenReview">RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation</a>
                    <a class="i-star" onclick="toggleAppStar('tirl2l9oKg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tirl2l9oKg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-XPe55Uffd7@OpenReview" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#XPe55Uffd7@OpenReview">Agnostic Active Learning Is Always Better Than Passive Learning</a>
                    <a class="i-star" onclick="toggleAppStar('XPe55Uffd7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XPe55Uffd7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-QN0E0KX2LM@OpenReview" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#QN0E0KX2LM@OpenReview">Learning Linear Attention in Polynomial Time</a>
                    <a class="i-star" onclick="toggleAppStar('QN0E0KX2LM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QN0E0KX2LM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-EoebmBe9fG@OpenReview" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#EoebmBe9fG@OpenReview">Optimal Mistake Bounds for Transductive Online Learning</a>
                    <a class="i-star" onclick="toggleAppStar('EoebmBe9fG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EoebmBe9fG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-rtG7n93Ru8@OpenReview" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#rtG7n93Ru8@OpenReview">State Entropy Regularization for Robust Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('rtG7n93Ru8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rtG7n93Ru8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-kVz9uvqUna@OpenReview" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#kVz9uvqUna@OpenReview">On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity</a>
                    <a class="i-star" onclick="toggleAppStar('kVz9uvqUna@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kVz9uvqUna@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-BSZqpqgqM0@OpenReview" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#BSZqpqgqM0@OpenReview">Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training</a>
                    <a class="i-star" onclick="toggleAppStar('BSZqpqgqM0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BSZqpqgqM0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-rMhQBlhh4c@OpenReview" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#rMhQBlhh4c@OpenReview">Adjoint Schrödinger Bridge Sampler</a>
                    <a class="i-star" onclick="toggleAppStar('rMhQBlhh4c@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rMhQBlhh4c@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-RxkCwOKVKa@OpenReview" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#RxkCwOKVKa@OpenReview">Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</a>
                    <a class="i-star" onclick="toggleAppStar('RxkCwOKVKa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RxkCwOKVKa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-UVDihUz0iT@OpenReview" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#UVDihUz0iT@OpenReview">High-Dimensional Calibration from Swap Regret</a>
                    <a class="i-star" onclick="toggleAppStar('UVDihUz0iT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UVDihUz0iT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-CH72XyZs4y@OpenReview" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#CH72XyZs4y@OpenReview">In Search of Adam’s Secret Sauce</a>
                    <a class="i-star" onclick="toggleAppStar('CH72XyZs4y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CH72XyZs4y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-U8BwT6Rmw4@OpenReview" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#U8BwT6Rmw4@OpenReview">An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds</a>
                    <a class="i-star" onclick="toggleAppStar('U8BwT6Rmw4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U8BwT6Rmw4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-efOq8wHH9o@OpenReview" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#efOq8wHH9o@OpenReview">MaxSup: Overcoming Representation Collapse in Label Smoothing</a>
                    <a class="i-star" onclick="toggleAppStar('efOq8wHH9o@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('efOq8wHH9o@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-IfD2MKTmWv@OpenReview" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#IfD2MKTmWv@OpenReview">Memory Mosaics at scale</a>
                    <a class="i-star" onclick="toggleAppStar('IfD2MKTmWv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IfD2MKTmWv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-jMhRbV47pS@OpenReview" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#jMhRbV47pS@OpenReview">The emergence of sparse attention: impact of data distribution and benefits of repetition</a>
                    <a class="i-star" onclick="toggleAppStar('jMhRbV47pS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jMhRbV47pS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-aLhA7AYLLR@OpenReview" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#aLhA7AYLLR@OpenReview">ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts</a>
                    <a class="i-star" onclick="toggleAppStar('aLhA7AYLLR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aLhA7AYLLR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-MrUsZfQ9pC@OpenReview" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#MrUsZfQ9pC@OpenReview">Identifiability of Deep Polynomial Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('MrUsZfQ9pC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MrUsZfQ9pC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Q3qAsZAEZw@OpenReview" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#Q3qAsZAEZw@OpenReview">Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference</a>
                    <a class="i-star" onclick="toggleAppStar('Q3qAsZAEZw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Q3qAsZAEZw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-4xvE6Iy77Y@OpenReview" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#4xvE6Iy77Y@OpenReview">PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('4xvE6Iy77Y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4xvE6Iy77Y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-R73ybUciQF@OpenReview" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#R73ybUciQF@OpenReview">A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</a>
                    <a class="i-star" onclick="toggleAppStar('R73ybUciQF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('R73ybUciQF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-B6bE2GC71a@OpenReview" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#B6bE2GC71a@OpenReview">EvoLM: In Search of Lost Language Model Training Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('B6bE2GC71a@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('B6bE2GC71a@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-WhEPg4mUs6@OpenReview" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#WhEPg4mUs6@OpenReview">Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</a>
                    <a class="i-star" onclick="toggleAppStar('WhEPg4mUs6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WhEPg4mUs6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-zJdutIT6vT@OpenReview" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#zJdutIT6vT@OpenReview">Discovering Opinion Intervals from Conflicts in Signed Graphs</a>
                    <a class="i-star" onclick="toggleAppStar('zJdutIT6vT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zJdutIT6vT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8P3QNSckMp@OpenReview" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#8P3QNSckMp@OpenReview">A Clean Slate for Offline Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('8P3QNSckMp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8P3QNSckMp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-F0JzotXYgC@OpenReview" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#F0JzotXYgC@OpenReview">Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy</a>
                    <a class="i-star" onclick="toggleAppStar('F0JzotXYgC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('F0JzotXYgC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gxfusMqPIs@OpenReview" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#gxfusMqPIs@OpenReview">Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('gxfusMqPIs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gxfusMqPIs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eIDa6pd9iQ@OpenReview" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#eIDa6pd9iQ@OpenReview">Auto-Compressing Networks</a>
                    <a class="i-star" onclick="toggleAppStar('eIDa6pd9iQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eIDa6pd9iQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oJ84bedrtM@OpenReview" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#oJ84bedrtM@OpenReview">MokA: Multimodal Low-Rank Adaptation for MLLMs</a>
                    <a class="i-star" onclick="toggleAppStar('oJ84bedrtM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oJ84bedrtM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-iydmH9boLb@OpenReview" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#iydmH9boLb@OpenReview">Advancing Expert Specialization for Better MoE</a>
                    <a class="i-star" onclick="toggleAppStar('iydmH9boLb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('iydmH9boLb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gm5mkiTGOy@OpenReview" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#gm5mkiTGOy@OpenReview">From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('gm5mkiTGOy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gm5mkiTGOy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KnqiC0znVF@OpenReview" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#KnqiC0znVF@OpenReview">Large Language Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('KnqiC0znVF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KnqiC0znVF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qYkhCah8OZ@OpenReview" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#qYkhCah8OZ@OpenReview">Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation</a>
                    <a class="i-star" onclick="toggleAppStar('qYkhCah8OZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qYkhCah8OZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fohuurA03P@OpenReview" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#fohuurA03P@OpenReview">Interactive Cross-modal Learning for Text-3D Scene Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('fohuurA03P@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fohuurA03P@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XoN10bZtR9@OpenReview" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#XoN10bZtR9@OpenReview">Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('XoN10bZtR9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XoN10bZtR9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OzdAnGHEPx@OpenReview" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#OzdAnGHEPx@OpenReview">Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables</a>
                    <a class="i-star" onclick="toggleAppStar('OzdAnGHEPx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OzdAnGHEPx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ImpizBSKcu@OpenReview" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#ImpizBSKcu@OpenReview">Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks</a>
                    <a class="i-star" onclick="toggleAppStar('ImpizBSKcu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ImpizBSKcu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-s0JVsx3bx1@OpenReview" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#s0JVsx3bx1@OpenReview">1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</a>
                    <a class="i-star" onclick="toggleAppStar('s0JVsx3bx1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('s0JVsx3bx1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XO9fhSZkBh@OpenReview" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#XO9fhSZkBh@OpenReview">Depth-Bounds for Neural Networks via the Braid Arrangement</a>
                    <a class="i-star" onclick="toggleAppStar('XO9fhSZkBh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XO9fhSZkBh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VYLdKb5dzO@OpenReview" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#VYLdKb5dzO@OpenReview">Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization</a>
                    <a class="i-star" onclick="toggleAppStar('VYLdKb5dzO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VYLdKb5dzO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sYK4yPDuT1@OpenReview" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#sYK4yPDuT1@OpenReview">A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('sYK4yPDuT1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sYK4yPDuT1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cGks3s79hW@OpenReview" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#cGks3s79hW@OpenReview">High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model</a>
                    <a class="i-star" onclick="toggleAppStar('cGks3s79hW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cGks3s79hW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oGmROC4e4W@OpenReview" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#oGmROC4e4W@OpenReview">Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('oGmROC4e4W@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oGmROC4e4W@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aUAG1WS7J2@OpenReview" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#aUAG1WS7J2@OpenReview">Class-wise Balancing Data Replay for Federated Class-Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('aUAG1WS7J2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aUAG1WS7J2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-m7MD0sa8Re@OpenReview" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#m7MD0sa8Re@OpenReview">Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</a>
                    <a class="i-star" onclick="toggleAppStar('m7MD0sa8Re@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('m7MD0sa8Re@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zd6VyjmN1S@OpenReview" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#Zd6VyjmN1S@OpenReview">ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism</a>
                    <a class="i-star" onclick="toggleAppStar('Zd6VyjmN1S@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zd6VyjmN1S@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7AwFJzgIUW@OpenReview" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#7AwFJzgIUW@OpenReview">Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks</a>
                    <a class="i-star" onclick="toggleAppStar('7AwFJzgIUW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7AwFJzgIUW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZwCVFBFUFb@OpenReview" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#ZwCVFBFUFb@OpenReview">QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</a>
                    <a class="i-star" onclick="toggleAppStar('ZwCVFBFUFb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZwCVFBFUFb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1b7whO4SfY@OpenReview" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#1b7whO4SfY@OpenReview">Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free</a>
                    <a class="i-star" onclick="toggleAppStar('1b7whO4SfY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1b7whO4SfY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-w1ihNiIBOc@OpenReview" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#w1ihNiIBOc@OpenReview">Learning long range dependencies through time reversal symmetry breaking</a>
                    <a class="i-star" onclick="toggleAppStar('w1ihNiIBOc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('w1ihNiIBOc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WJujF9An5L@OpenReview" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#WJujF9An5L@OpenReview">FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('WJujF9An5L@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WJujF9An5L@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-knPz7gtjPW@OpenReview" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#knPz7gtjPW@OpenReview">Superposition Yields Robust Neural Scaling</a>
                    <a class="i-star" onclick="toggleAppStar('knPz7gtjPW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('knPz7gtjPW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-i5WnXNjwbR@OpenReview" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#i5WnXNjwbR@OpenReview">ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression</a>
                    <a class="i-star" onclick="toggleAppStar('i5WnXNjwbR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('i5WnXNjwbR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RF3miSqdXa@OpenReview" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#RF3miSqdXa@OpenReview">On Linear Mode Connectivity of Mixture-of-Experts Architectures</a>
                    <a class="i-star" onclick="toggleAppStar('RF3miSqdXa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RF3miSqdXa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0biUwyjKkm@OpenReview" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#0biUwyjKkm@OpenReview">OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model</a>
                    <a class="i-star" onclick="toggleAppStar('0biUwyjKkm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0biUwyjKkm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-koEALFNBj1@OpenReview" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#koEALFNBj1@OpenReview">Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think</a>
                    <a class="i-star" onclick="toggleAppStar('koEALFNBj1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('koEALFNBj1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-s6k9l5yX8e@OpenReview" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#s6k9l5yX8e@OpenReview">Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation</a>
                    <a class="i-star" onclick="toggleAppStar('s6k9l5yX8e@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('s6k9l5yX8e@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NM4emKloy6@OpenReview" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#NM4emKloy6@OpenReview">Learning (Approximately) Equivariant Networks via Constrained Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('NM4emKloy6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NM4emKloy6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jRXgRC6fu7@OpenReview" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#jRXgRC6fu7@OpenReview">SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding</a>
                    <a class="i-star" onclick="toggleAppStar('jRXgRC6fu7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jRXgRC6fu7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4OsgYD7em5@OpenReview" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#4OsgYD7em5@OpenReview">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a>
                    <a class="i-star" onclick="toggleAppStar('4OsgYD7em5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4OsgYD7em5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-s6YHno8Ke3@OpenReview" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#s6YHno8Ke3@OpenReview">Learning to Learn with Contrastive Meta-Objective</a>
                    <a class="i-star" onclick="toggleAppStar('s6YHno8Ke3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('s6YHno8Ke3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-JFygzwx8SJ@OpenReview" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#JFygzwx8SJ@OpenReview">KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('JFygzwx8SJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JFygzwx8SJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NM8Apk61NA@OpenReview" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#NM8Apk61NA@OpenReview">HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('NM8Apk61NA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NM8Apk61NA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zwCb9cKHpd@OpenReview" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#zwCb9cKHpd@OpenReview">SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing</a>
                    <a class="i-star" onclick="toggleAppStar('zwCb9cKHpd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zwCb9cKHpd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WCRPgBpbcA@OpenReview" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#WCRPgBpbcA@OpenReview">A multiscale analysis of mean-field transformers in the moderate interaction regime</a>
                    <a class="i-star" onclick="toggleAppStar('WCRPgBpbcA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WCRPgBpbcA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CaSQgef484@OpenReview" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#CaSQgef484@OpenReview">Exploring Diffusion Transformer Designs via Grafting</a>
                    <a class="i-star" onclick="toggleAppStar('CaSQgef484@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CaSQgef484@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rMdf8jhLR7@OpenReview" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#rMdf8jhLR7@OpenReview">Generalized Gradient Norm Clipping &amp; Non-Euclidean $(L_0,L_1)$-Smoothness</a>
                    <a class="i-star" onclick="toggleAppStar('rMdf8jhLR7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rMdf8jhLR7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Q6IyUpBmrG@OpenReview" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#Q6IyUpBmrG@OpenReview">Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion</a>
                    <a class="i-star" onclick="toggleAppStar('Q6IyUpBmrG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Q6IyUpBmrG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XQ87Vo9GIz@OpenReview" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#XQ87Vo9GIz@OpenReview">TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability</a>
                    <a class="i-star" onclick="toggleAppStar('XQ87Vo9GIz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XQ87Vo9GIz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zIzZxDsNNP@OpenReview" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#zIzZxDsNNP@OpenReview">PhySense: Sensor Placement Optimization for Accurate Physics Sensing</a>
                    <a class="i-star" onclick="toggleAppStar('zIzZxDsNNP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zIzZxDsNNP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-JcEqp4aPmb@OpenReview" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#JcEqp4aPmb@OpenReview">InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation</a>
                    <a class="i-star" onclick="toggleAppStar('JcEqp4aPmb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JcEqp4aPmb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gL4muAFwsh@OpenReview" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#gL4muAFwsh@OpenReview">Does Stochastic Gradient really succeed for bandits?</a>
                    <a class="i-star" onclick="toggleAppStar('gL4muAFwsh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gL4muAFwsh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-INqBOmwIpG@OpenReview" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#INqBOmwIpG@OpenReview">Perception Encoder: The best visual embeddings are not at the output of the network</a>
                    <a class="i-star" onclick="toggleAppStar('INqBOmwIpG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('INqBOmwIpG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gq4Gay8rDB@OpenReview" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#Gq4Gay8rDB@OpenReview">PlayerOne: Egocentric World Simulator</a>
                    <a class="i-star" onclick="toggleAppStar('Gq4Gay8rDB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gq4Gay8rDB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uWj4s7rMnR@OpenReview" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#uWj4s7rMnR@OpenReview">Mean Flows for One-step Generative Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('uWj4s7rMnR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uWj4s7rMnR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Main-bold, sans-serif;"></div></div></body></html>